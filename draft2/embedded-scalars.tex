\documentclass[leqno,12pt]{article}

\input{preamble}

\hypersetup{
  pdfauthor={Christopher Potts, Daniel Lassiter, Roger Levy, and Michael C. Frank},
  pdftitle={Embedded implicatures as pragmatic inferences under compositional lexical uncertainty},
  pdfsubject={computational pragmatics},
  pdfkeywords={conversational implicatures, scalar implicatures, embedded implicatures, Bayesian pragmatics, experimental pragmatics}
}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Embedded implicatures as pragmatic inferences under compositional lexical uncertainty%
  \thanks{All the data and code used in this paper are available at \url{https://github.com/cgpotts/pypragmods}}}
\author{Christopher Potts, Daniel Lassiter, Roger Levy, and Michael C.~Frank}
\maketitle

\marginnote{Light revision to abstract in response to objections that it's not about what's possible but rather about what's marked.}

\begin{abstract} 
  How do comprehenders reason about pragmatically ambiguous scalar
  terms like \word{some} in complex syntactic contexts?  In many
  pragmatic theories of conversational implicature, local
  exhaustification of such terms (`only some') is predicted to be
  marked or impossible if it does not strengthen the literal meaning,
  whereas grammatical accounts predict such construals to be robustly
  available. Recent experimental evidence supports the salience of
  these local enrichments, but the grammatical theories that this
  evidence supports do not provide viable mechanisms for weighting
  such construals against others. We propose a probabilistic model
  that combines previous work on pragmatic inference under `lexical
  uncertainty' with a more detailed model of compositional
  semantics. We show that this model makes accurate predictions about
  new experimental data on embedded implicatures in both non-monotonic
  and downward-entailing semantic contexts. In addition, the model's
  predictions can be improved by the incorporation of neo-Gricean
  hypotheses about lexical alternatives. This work thus contributes to
  a synthesis of grammatical and probabilistic views on pragmatic
  inference.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conversational implicature: Interacting with grammar}\label{sec:introduction}

The linguistic forms that discourse participants exchange with each
other routinely underrepresent the speaker's intended message and
underdetermine the listener's inferences. \citet{Grice75} famously
provided a philosophical framework for understanding the driving
forces behind such pragmatic enrichment. At the heart of this
framework are \tech{conversational implicatures}: social, cognitively
complex meanings that discourse participants create jointly in
interaction.

Perhaps the best-studied examples of language users going beyond the
literal semantics involve weak terms like \word{some} being
strengthened to exclude their communicatively stronger alternatives,
giving rise to construals like `some and not all' or `only some'.
Such inferences are often called \tech{scalar conversational
  implicatures} (SIs), and they are widely assumed to arise via the
same social inferencing mechanisms that are at work in other
implicatures.  However, this assumption has always been
controversial. Even \citeauthor{Grice75} suggested that SIs might be
closer to the grammar than other implicatures (p.~56; see also
\citealt{Levinson00,Sperber95,Bach06}), and recent grammar-driven
accounts are framed in direct opposition to an implicature analysis.
For example, \pgcitet{ChierchiaFoxSpector08}{2316} write, ``the facts
suggest that SIs are not pragmatic in nature but arise, instead, as a
consequence of semantic or syntactic mechanisms''. The ensuing debates
have stimulated new insights, pushing researchers to identify and
evaluate previously unnoticed consequences of the two broad positions.

Much of the debate between Gricean and grammar-driven accounts has
centered around what we informally called \tech{embedded implicatures}
--- cases where a pragmatically enriched interpretation seems to be
incorporated into the compositional semantics. Such readings seem
initially to demand implicature-enriched semantic representations.
However, many of the relevant examples have received straightforward
Gricean accounts in which semantic content and contextual assumptions
interact to yield global implicatures that are meaning-equivalent to
interpretations that would derive from local pragmatic enrichment
\citep{Russell06,Geurts09}. This reduces the power of such examples to
decide in favor of one side or the other.

\marginnote{Light revisions to try to answer reviewer objections about framing.}

\citet{Geurts:Pouscoulous:2009} and \citet{Chemla:Spector:2011} study
weak scalar terms in a wide range of quantificational environments.
They show that many of the attested listener inferences are amenable
to Gricean treatments based on implicature calculation, with no need
for the such calculations to intrude on the semantics. However, they
identify a class of examples that, if attested, would not admit such a
treatment: scalar terms in the scope of non-monotone quantifiers, as
in \word{Exactly one player hit some of his shots}. In such cases, the
interpretation enriched by an embedded implicature (\word{\ldots some
  but not all of his shots}) does not entail the literal meaning,
whereas the Gricean implicature analysis of scalar terms can only
strengthen literal meanings. \citeauthor{Geurts:Pouscoulous:2009}'s
experiments fail to support enrichment in such contexts, whereas
\citeauthor{Chemla:Spector:2011}'s suggest that it is possible. A
number of recent papers have sought to make sense of these conflicting
results \citep{Clifton:Dube:2010,geurts-vantiel:2013:scalar,vanTiel:2014}.

\marginnote{More revisions to try to answer reviewer objections about framing.}

In this paper, we reproduce the central qualitative result of
\citet{Chemla:Spector:2011} using more naturalistic experimental
stimuli, a fully randomized between-subjects design to avoid unwanted
inferences across critical items \citep{geurts-vantiel:2013:scalar},
and a more direct method of interpreting participants' responses. Like
\citeauthor{Chemla:Spector:2011}, we find that scalar terms in
non-monotone environments support implicature inferences (though these
seem not to be the preferred or most salient construals).  In our
view, this evidence points to a role for compositional semantics in
understanding implicatures. But pragmatic factors also play a crucial
role: even embedded implicatures arise only with proper contextual
support
\citep{Russell06,ChierchiaFoxSpector08,Geurts:2011,Chemla:Spector:2011}.

To describe the complementary roles of grammar and pragmatics in
embedded implicatures, we propose a model that both embraces the
compositional insights of \citeauthor{ChierchiaFoxSpector08} and
characterizes how people arrive at such construals. This model is in
the tradition of \tech{rational speech act} models
\citep{Frank:Goodman:2012,Goodman:Stuhlmuller:2013} and \tech{iterated
  best response} models \citep{Franke09DISS,Jaeger:2011}, and is a
direct extension of the \tech{compositional lexical uncertainty} model
of \citet{Bergen:Goodman:Levy:2012} and
\citet{Bergen:Levy:Goodman:2014}. The model accounts for how discourse
participants coordinate on the right logical forms (implicature-rich
or not), seeking to retain the insights of Gricean accounts while
paying close attention to the details of semantic composition.

\marginnote{Mid-paragraph, added a note about the limited nature of our understanding of the correct refinements.}

We show that our model not only captures the qualitative pattern of
implicature behaviors that \citeauthor{Chemla:Spector:2011} found, but
also makes quantitative predictions that are highly correlated with
people's actual inferential behavior in context. In addition, we
present evidence that these correlations can be improved if the set of
refinements is lexically constrained, in keeping with broadly
neo-Gricean views of scalar implicature
\citep{Horn72,Gazdar79b,Gazdar79a,SchulzVanRooij06}, though the
precise nature of the true refinements remains a challenging open
question.  Our results suggest that the full theory of implicature
depends substantively on the fine details of semantic composition
\emph{and} broader considerations of rational interaction. This is
perhaps a departure from \posscitet{Grice75} particular conception of
pragmatic meaning, but it is well-aligned with his general theory of
meaning and intention \citep{Grice89}.  In view of our experimental
results, the chief advantage of our model is that it makes
quantitative predictions that are easily and rigorously linked with
our human response patterns. In other words, the model makes
predictions not only about which pragmatic inferences are possible but
also about how likely those inferences are.

Our broader position is that grammar-driven accounts and Gricean
accounts are not in opposition, but rather offer complementary
insights.  When communicating in natural languages, people are relying
on linguistic conventions to try to identify and convey each other's
intentions. All sides in the debate acknowledge this mix of
grammatical and interactional factors. \posscitet{Grice75} definition
of conversational implicature is interactional, but his maxim of
manner embraces a role for language. By introducing additional devices
such as Horn scales, Neo-Griceans expand this role into areas Grice
addressed with the maxims of quantity, quality, and
relevance. \citet{Sperber95} and \citet{Bach94} characterize many
kinds of pragmatic enrichment as inferences about logical forms. And
\citet{ChierchiaFoxSpector08} invoke broadly Gricean pressures to
explain how speakers and listeners coordinate on whether to posit
implicature-rich logical forms or more literal ones. Thus, there is
substantially more consensus than the rhetoric often suggests.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implicature, enrichment, and embedding}\label{sec:implicature}

\marginnote{Light revisions throughout this section to avoid characterizations in terms of `Griceans', etc.}

In this section, we describe embedded implicatures, seeking to
identify the special theoretical challenges that they pose.  Under
\posscitet{Grice75} original definition, conversational implicature is
an act of social cognition. The original definition is somewhat
underspecified, and fleshing it out into a precise formulation is
challenging \citep{Hirschberg85}, but the guiding idea seems clear.
The listener assumes that the speaker is cooperative in the Gricean
sense of rational interaction. However, the listener is confronted
with an utterance $U$ with content $p$ that meets this assumption only
if certain additional conditions are met. The listener can resolve
this tension by positing that these conditions are in fact met; in
many (but not all) cases, this means inferring that the speaker
intended for the listener to infer the truth of an additional
proposition $q$, or of a different but related proposition $q'$. By
this reasoning, the listener is able to reconcile the observation that
the speaker chose to utter $U$ with the assumption that the speaker is
communicating cooperatively.

In the current work, we do not try to make the above description more
rigorous. The model that we develop does not depend on an
independently formulated definition of implicature, but rather seeks
to derive such meanings from more basic considerations about how
speakers and listeners reason about each other whenever they
interact. Similarly, the model of \citet{ChierchiaFoxSpector08} is
noncommittal about the reality of conversational implicatures per
se. For them, `conversational implicature' can be seen as an informal
label for a certain class of logical forms, rather than a conceptual
primitive (see \secref{sec:cfs} of this paper). With this in mind, we
use the notion of conversational implicature only to articulate the
central empirical focus of this paper --- embedded scalar terms ---
and the associated challenges for formal pragmatic accounts.

On the classic Gricean account, scalar implicatures arise when the
imperative `Be as informative as is required' (a subclause of the
maxim of quantity) is in tension with another pragmatic pressure
related to cooperative communication. The opposing force can take many
forms, for example, relating to considerations of politeness,
discretion, or secrecy, but it is usually attributed to the maxim of
quality, which instructs speakers to say only what they have strong
positive evidence for. For instance, imagine a sportscaster who has
observed the outcome of a single round of a basketball tournament and
is reporting on it as news. If the sportscaster says \eg{some}, then
she will likely implicate that Player~A did not make all of his shots.
%
\begin{examples}
\item\label{some} Player~A hit some of his shots.
\end{examples}

The scalar implicature follows from a straightforward application of
the above ideas. We assume the sportscaster is cooperative in the
Gricean sense, and knowledgeable and forthcoming about the
events. Why, then, did she opt for a weak statement like
\word{Player~A hit some of his shots} when a stronger statement like
\word{Player~A hit all of his shots} is available and would have been
more informative?  If knowledge is the only relevant consideration, it
must be that she was prevented from using this stronger form because
she does not know it to be true. Together with our assumption that she
observed the full outcome, she can lack knowledge of this proposition
only because it is false, leading to the implicated meaning that
Player~A did not hit all of his shots. In this way, a listener can
enrich the speaker's message.

To make this concrete, suppose that we have two players, A and B, and
that we care (for present purposes) only about whether each of them
hit none, some but not all, or all of his shots. We can identify these
(equivalence classes of) possible worlds with labels like \world{NA},
which means that Player~A hit none of his shots and Player~B hit all
of his shots, and \world{SS}, which means that both players hit some
but not all of their shots. There are $3^{2} = 9$ such worlds. The
literal semantics of \eg{some} in this context is the proposition
given in (\ref{some-sem}b). Our hypothesized implicature is
(\ref{some-sem}c), the proposition that Player~A did not hit all of
his shots.  The intersection of these two meanings delivers the
communicated meaning, (\ref{some-sem}d).
%
\begin{examples}
\item\label{some-sem}
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}[t]{@{} r@{. \ } l *{9}{c}@{\hspace{18pt}} l}
    a& Worlds:       & \world{NN} & \world{NS} & \world{NA} & \world{SN} & \world{SS} & \world{SA} & \world{AN} & \world{AS} & \world{AA} & \\
    b& Literal:      &            &            &            & \world{SN} & \world{SS} & \world{SA} & \world{AN} & \world{AS} & \world{AA} & `at least some'\\
    c& Implicature:  & \world{NN} & \world{NS} & \world{NA} & \world{SN} & \world{SS} & \world{SA} &            &            &            & `not all' \\
    d& Communicated: &            &            &            & \world{SN} & \world{SS} & \world{SA} &            &            &            & `only some'
  \end{tabular}
\end{examples}

There are many proposals for how to formalize this reasoning. The
common theme running through all of them is that the implicature is
accessible because it is a refinement that strictly entails the
original literal content --- in this example, because the utterance's
literal meaning and the implicature are combined into an enriched
meaning by intersection. In \citeauthor{Grice75}'s terms, a general
claim is further restricted by the interaction of quantity and
quality.

The above reasoning extends to examples like \eg{everysome}, in which
\word{some} is in the scope of a universal quantifier, though
additional assumptions must be brought in to achieve a comparable
implicature.
%
\begin{examples}
\item\label{everysome} Every player hit some of his shots.
\end{examples}
%
Consider the potential enrichment of this sentence to convey that
every player hit some but not all of his shots. This seems comparable
to the construal we derived for \eg{some}, but it requires more
assumptions.  If we take the implicature to be the negation of the
stronger alternative \word{every player hit all of his shots}, then
the reasoning proceeds as in the first four lines of
\eg{everysome-sem}, which takes us to a meaning (\ref{everysome-sem}d)
that is consistent with one or the other of the players (but not both)
having hit all of his shots. To arrive at the target meaning (every
player hit some but not all of his shots), we must further assume an
auxiliary premise beyond that required for \eg{some}. One example of
such a premise is that of uniform outcomes (\ref{everysome-sem}e);
there are many others that will do the job
\citep{Spector:2007:SCALAR}.
%
\begin{examples}
\item\label{everysome-sem}
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}[t]{@{} r@{. \ }l *{9}{c} @{\hspace{18pt}} l }
    a & Worlds:         & \world{NN} & \world{NS} & \world{NA} & \world{SN} & \world{SS} & \world{SA} & \world{AN} & \world{AS} & \world{AA} \\
    b & Literal:        &            &            &            &            & \world{SS} & \world{SA} &            & \world{AS} & \world{AA} & `all hit at least some' \\ 
    c & Implicature:    & \world{NN} & \world{NS} & \world{NA} & \world{SN} & \world{SS} & \world{SA} & \world{AN} & \world{AS} &            & `not all hit all' \\
    d & Result:         &            &            &            &            & \world{SS} & \world{SA} &            & \world{AS} &            & `all hit some; not all hit all'\\    
    e & Aux.~premise:   & \world{NN} &            &            &            & \world{SS} &            &            &            & \world{AA} & `uniform outcomes' \\
    f & Communicated:   &            &            &            &            & \world{SS} &            &            &            &            & `all hit only some'
  \end{tabular}
\end{examples}

Though the need for an auxiliary premise is a noteworthy complication,
it seems within the bounds of a Gricean account, and auxiliary
premises like these might be independently justified
\citep{Russell06}. As in the previous example, the communicated
meaning is an enrichment of the literal content, and Gricean pressures
and contextual assumptions deliver the stronger
meaning. \citet{Geurts:Pouscoulous:2009} and
\citet{Chemla:Spector:2011} home in on this common theme in scalar
implicature calculation and use it to probe the scope and adequacy of
the Gricean implicature framework. Examples like \eg{exactlyonesome}
drive their discussions.  This is a minimal variant of \eg{everysome}
with the subject universal determiner \word{every} replaced by
\word{exactly one}.
%
\begin{examples}
\item\label{exactlyonesome} Exactly one player hit some of his shots.
\end{examples}

Many people have the intuition that \eg{exactlyonesome} can be used to
describe a situation in which there is exactly one player who scored
some but not all of his shots, which is consistent with some players
having scored all of their shots. The reading is easy to characterize
intuitively: one imagines that \word{some of his shots} has been
locally enriched to \word{some but not all of his shots}, and that
this enriched meaning is the semantic argument to the subject
quantifier. What makes this reading notably different from, e.g.,
\eg{everysome} is that it does not entail the literal reading, as we
see in \eg{exactlyonesome-sem}. The literal semantics is the
proposition in (\ref{exactlyonesome-sem}b), whereas the content of the
\word{\ldots some but not all of his shots} (`Local') construal is
(\ref{exactlyonesome-sem}c), which merely overlaps with it.
%
\begin{examples}
\item\label{exactlyonesome-sem}
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}[t]{@{} r@{. \ } l *{9}{c}@{\hspace{18pt}} l}
    a& Worlds:       & \world{NN} & \world{NS} & \world{NA} & \world{SN} & \world{SS} & \world{SA} & \world{AN} & \world{AS} & \world{AA} & \\
    b& Literal:      &            & \world{NS} & \world{NA} & \world{SN} &            &            & \world{AN} &            &            & `exactly one hit at least some'\\
    c& Local:        &            & \world{NS} &            & \world{SN} &            & \world{SA} &            & \world{AS} &            & `exactly one hit only some' \\
  \end{tabular}
\end{examples}
%
Any theory in which enriched scalar interpretations are always
generated by intersection, as they are in classical Gricean and
neo-Gricean accounts, will fail to arrive at
(\ref{exactlyonesome-sem}c). Such theories head inexorably toward a
refinement that excludes \world{NA} and \world{AN}, but they are
essentially incapable of `introducing' \world{SA} and \world{AS}.
If such construals are possible, they must arise from other
mechanisms.

\marginnote{Minor revisions to quietly acknowledge other potential Gricean routes to such meanings.}

The issue is even clearer when a scalar term is in the scope of a
downward-monotone operator like \word{no}, as in \word{no player hit
  some of his shots}. In such cases, the embedded enrichment creates a
meaning that is strictly entailed by (weaker than) the literal
meaning:
%
\begin{examples}
\item\label{nosome-sem}
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}[t]{@{} r@{. \ } l *{9}{c}@{\hspace{18pt}} l}
    a& Worlds:       & \world{NN} & \world{NS} & \world{NA} & \world{SN} & \world{SS} & \world{SA} & \world{AN} & \world{AS} & \world{AA} & \\
    b& Literal:      & \world{NN} &            &            &            &            &            &            &            &            & `none hit some' \\
    c& Local:        & \world{NN} &            & \world{NA} &            &            &            & \world{AN} &            & \world{AA} & `none hit only some' \\
  \end{tabular}
\end{examples}

Gricean theories predict that the `local' enrichment of \word{some} to
\word{only some} is unavailable as an implicature inference here,
either because of the way pragmatic pressures interact or because
\word{some} is held to be the strongest member of its scale in
negative environments, leaving no room for further
enrichment. Grammar-driven approaches have tended to agree with the
basic empirical assumption, arguing that local enrichment is blocked
in environments where it would strictly weaken the literal content
\citep{chierchia2006broaden}.
 
The empirical evidence is mixed but seems to support the accessibility
of these local interpretations. Modifying an earlier design by
\citet{Geurts:Pouscoulous:2009}, \citeauthor{Chemla:Spector:2011} used
displays involving geometric patterns to assess whether interpreters
could access local-enrichment readings of scalar terms in the scope of
non-monotone and downward-monotone operators. Their findings suggest
that local enrichment readings are available in both contexts,
especially non-monotone ones. Skeptics of local enrichment have found
grounds for challenging \citeauthor{Chemla:Spector:2011}'s findings
(see \secref{sec:lit}), but we believe that the theoretical challenges
posed by embedded implicatures are real. In \secref{sec:binary}, we
describe a new experiment that reproduces the core qualitative
findings of \citeauthor{Chemla:Spector:2011}'s studies.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\CFS's grammar-driven model}\label{sec:cfs}

\marginnote{This section is unchanged. I believe no reviewers complained about it.}

This section briefly reviews the grammar-driven model of
\citet{ChierchiaFoxSpector08} (henceforth \CFS).  The approach is
inspired by those of \citet{Chierchia01}, \citet{Sauerland01},
\citet{Spector:2007}, and \citet{Fox:2007,Fox:2009}. There are two
central pieces to the account: a generally available function $\ALT$
that maps words and phrases to their alternatives, and a covert
exhaustification operator $O$.

For $\ALT$, the relevant notion of alternative is familiar from
theories of questions and focus \citep{Groenendijk84,Rooth85,Rooth92}:
we can assume, as a default, that the alternatives for an expression
$\varphi$ is some subset of the items in the same type-theoretic
denotation domain as $\sem{\varphi}$, the meaning of $\varphi$.  The
precise value of the function $\ALT$ is context-dependent, and
discourse participants are presumed to coordinate on it, just as they
coordinate on the meanings of deictic or discourse-bound pronouns,
elided phrases, and other pragmatically controlled free variables.

The effect of applying the basic exhaustification operator $O$ to an
expression $\varphi$ in the context of a given $\ALT$ is shown in
\eg{def:O}
\citep{Spector:2007,Fox:2007,Fox:2009,Magri:2009,ChierchiaFoxSpector08}.\footnote{This
  is not the operator that \CFS\ ultimately favor, since it
  requires some implicit restrictions on allowable $\ALT$ functions in
  order to get the right inferences.  The final version has the same
  form as \eg{def:O} but further restricts $\ALT$.}
%
\begin{examples}
\item\label{def:O}
  $\OALT(\varphi) = 
  \sem{\varphi} \sqcap \bigsqcap\set{ -q : q \in \ALT(\varphi) \wedge \sem{\varphi} \not\entails q}$ 
\end{examples}
%
The $O$ operator maps an expression $\varphi$ to one that entails
$\sem{\varphi}$ and excludes the denotations of expressions in
$\ALT(\varphi)$ that are strictly stronger than $\sem{\varphi}$. When
dealing with truth-functional expressions, we can regard $\sqcap$ as
boolean conjunction and $\entails$ as a material conditional, but the
definition should be thought of as broad enough to include any kind of
partial ordering \seccitep{Hirschberg85}{4}.

Part of the case for a grammar-driven view is that it uses pieces of
semantic theory that are independently needed. In particular,
exhaustification is at the heart of \posscitet{Groenendijk84} theory
of questions and their answers. The above operator is a common
proposal for the meaning of \word{only} (for discussion:
\citealt{Rooth96,Buring01,BeaverClark08}).  \citet{SchulzVanRooij06}
use exhaustification for implicature calculation (see also
\citealt{deJagerVanRooij07}).  (For critical discussion, see
\citealt{Alonso-Ovalle:2008} and \citealt{Gajewski:2012}.) While \CFS\
are cautious about making direction connections between $O$ and these
other phenomena (p.~2304), the correspondences are nonetheless
noteworthy.

Those are the technical pieces. The proposal can then be summarized
easily: $O$ operators can optionally appear anywhere in the logical form of a
sentence, perhaps subject to additional restrictions and general
preferences (see \CFS: $\S$4.6). To see the effects that this could
have, let's return to the examples involving \word{some} that we
reviewed in \secref{sec:implicature}. Simplifying slightly, let's
suppose that \word{some shot} denotes the set of sets  in \eg{someshot}
--- the set of all sets $Y$ that have a non-empty intersection with the
set of shots.
%
\begin{examples}
\item\label{someshot} $\sem{\word{some shot}} = \set{Y : \sem{\word{shot}} \cap Y \neq \emptyset}$
\end{examples}
%
Consider a domain of three entities $\set{a,b,c}$, and assume that
$\sem{\word{shot}} = \set{a,b}$. Then the above is equivalent to the
set of sets contained in the green box in \figref{fig:qspace}. Now suppose that
$\ALT(\word{some shot})$ is defined as follows:
%
\begin{examples}
\item\label{altsome} $\ALT(\word{some shot}) =  
  \set{
    \sem{\word{some shot}}, 
    \sem{\word{every shot}}, 
    \sem{\word{no shot}}
  }$
  \begin{examples}
  \item $\sem{\word{some shot}}$ as in \eg{someshot} \hfill (green circle in \figref{fig:qspace})
  \item $\sem{\word{every shot}} = \set{Y : \sem{\word{shot}} \subseteq Y}$ \hfill (purple circle in \figref{fig:qspace})
  \item $\sem{\word{no shot}} = \set{Y : \sem{\word{shot}} \cap Y = \emptyset}$  \hfill (orange circle in \figref{fig:qspace})
  \end{examples}
\end{examples}

\begin{figure}[tp]
  \centering
  \newcommand{\labelednodeleft}[2]{\put(#1){\makebox(0,0)[l]{#2}}}
  \newcommand{\labelednode}[2]{\put(#1){\makebox(0,0){#2}}}
  \newcommand{\picline}[3]{\put(#1){\line(#2){#3}}}
  \setlength{\unitlength}{1.2cm}
  \begin{picture}(9.5,4.5)   
    \labelednode{2.75,3}{$\set{a,b,c}$}
        
    \picline{0.75,2.2}{3,1}{1.8}
    \picline{4.75,2.2}{-3,1}{1.8}
    \picline{2.75,2.2}{0,1}{0.6}
    \labelednode{0.5,2}{$\set{a,b}$}
    \labelednode{2.75,2}{$\set{a,c}$}
    \labelednode{5,2}{$\set{b,c}$}
    \picline{2.50,1.2}{-3,1}{1.8}
    \picline{2.75,1.2}{0,1}{0.6}
    \picline{3.00,1.2}{3,1}{1.8}    
    \picline{0.50,1.2}{0,1}{0.6}
    \picline{0.75,1.2}{3,1}{1.8}
    \picline{4.75,1.2}{-3,1}{1.8}
    \picline{5,   1.2}{0,2}{0.6}

    \picline{2.50,.2}{-3,1}{1.8}
    \picline{2.75,.2}{0,1}{0.6}
    \picline{3.00,.2}{3,1}{1.8}

    \labelednode{0.5,1}{$\set{a}$}
    \labelednode{2.75,1}{$\set{b}$}
    \labelednode{5,1}{$\set{c}$}
    \labelednode{2.75,0}{$\emptyset$}

    \linethickness{2pt}
    \rotatebox{20}{\put(3.75,-0.8){{\color{cborange}\oval(3.5,0.65)}}}
    \rotatebox{20}{\put(1.3,2.25){{\color{cbpurple}\oval(3.6,0.85)}}}
    \rotatebox{20}{\put(1.5,1.65){{\color{cbgreen}\oval(5.8,2.7)}}}
    \rotatebox{20}{\put(1,1.35){{\color{gray}\oval(5.5,1.48)}}}

    \labelednodeleft{4,4}{{\color{cbgreen}$\sem{\word{some shot\/}}$}}
    \labelednodeleft{4,3}{{\color{cbpurple}$\sem{\word{every shot\/}}$}}
    \labelednodeleft{4,2}{{\color{darkgray}$\sem{\OALT(\word{some shot\/})}$ as in \eg{altsome}}}
    \labelednodeleft{4,1}{{\color{cborange}$\sem{\word{no shot\/}}$}}

  \end{picture}

  \vspace{4pt}

  \caption{Given a domain $\set{a,b,c}$ with $\sem{\word{shot}} = \set{a,b}$,
    $\sem{\word{some shot}}$ is equal to the set of sets in the green box,
    $\sem{\word{every shot}}$ to the set of sets in the purple box, and
    $\sem{\word{no shot}}$ to the set of sets in the orange box. If $\OALT(\word{some shot})$ 
    contains  $\sem{\word{every shot}}$, then \word{some shot} is 
    refined to exclude the purple subset.}
  \label{fig:qspace}
\end{figure}

The presence of $\sem{\word{some shot\/}}$ has no effect because it is
identical to the input. Similarly, all quantifiers that are weaker
than the input have no effect if included in the $\ALT$ set. The
presence of $\sem{\word{no shot\/}}$ has no effect because it
contradicts the input, so its complement is weaker than the input.
The presence of $\sem{\word{every shot\/}}$ will, though, be meaningful,
as long as we assume that $\sem{\word{shot\/}} \neq \emptyset$.  In that
case, $\OALT(\word{some shot\/})$ will denote the subset in gray in
\figref{fig:qspace}.  This is equivalent to the intersection of
$\sem{\word{some shot\/}}$ and the complement of
$\sem{\word{every shot\/}}$ in the power set of the domain.  In other
words, it expresses \word{some and not all}, the intuitively
implicature-rich interpretation.

Because $\OALT$ is embeddable, syntactic constituents like
$\OALT(\word{some shot})$ can appear in the scope of quantifiers.
Implicature-rich versions of \eg{some}, \eg{everysome}, and
\eg{exactlyonesome} are thus available --- potentially usable by
speakers and inferable by listeners just like any other semantic
resolution for an underspecified form in context.

As we noted in the introduction, \CFS\ draw a firm rhetorical
distinction between their proposal and the Gricean approach to
pragmatics. They state, ``the goal of this paper is to challenge the
neo-Gricean approach to SIs'' (p.~2303), and, as we said, they later
write that ``the facts suggest that SIs are not pragmatic in nature
but arise, instead, as a consequence of semantic or syntactic
mechanisms'' (p.~2316). The sense in which their account reflects this
position is clear: to characterize implicatures, we need not consider
the interactional setting or try to model the speaker and
hearer. Rather, we can just describe a specific class of logical
forms.

However, this position is tempered by \CFS's pervasive appeals to
Gricean reasoning.  The authors' specific examples are generally
placed in contexts that support the target implicatures by ensuring
that they are relevant, informative, and truthful.  They concede that
``aspects of the Gricean picture are sound and effective''
(p.~2299). And, in summarizing their account, they make explicit the
role that Gricean pragmatics must play in helping discourse
participants to coordinate on the right logical forms:
%
\begin{quote}
  one can capture the correlation with various contextual
  considerations, under the standard assumption (discussed in the very
  beginning of this paper) that such considerations enter into the
  choice between competing representations (those that contain the
  operator and those that do not). (p.~2317)
\end{quote}

The coordination problem that \citeauthor{Grice75} sought to solve
therefore remains, in the following form. First, in \CFS's theory, the
discourse participants must coordinate on the nature of the
function $\ALT$.  Second, because the language permits but does not
require silent, embedded $O$ operators in many positions, the
speaker's signal frequently underdetermines her intended message; a
given surface form $U$ might be consistent with logical forms that
encode implicatures and those that don't, depending on where $O$
appeared. Crucially, the speaker must rely on the listener to select
the right one.  Overall, then, implicature calculation now amounts to
reasoning about which logical form was intended. How this coordination
happens has not been a focus of grammar-driven accounts, but the above
quotation suggests that communicative pressures like those
\citeauthor{Grice75} identified guide the process.

Summarizing so far, we have evidence from
\posscitet{Chemla:Spector:2011} experiments that some implicatures
require, in some sense, local enrichment of embedded content via
enhanced logical forms. Traditional Gricean accounts seem unable to
capture such cases, but such accounts excel at characterizing how
speakers and listeners coordinate on implicatures in simpler
cases. \CFS, in contrast, define a model in which local calculation is
immediate, but they do not venture an account of how discourse
participants coordinate on the right logical forms when more than one
is allowed by the grammar. Stepping back, we see that both the Gricean
and grammar-driven accounts clearly have something to contribute. We
now turn to the task of developing a synthesis of the two approaches:
a model that formally implements pragmatic reasoning over complex,
compositionally defined logical forms and that is able to achieve the
readings that seem to demand local enrichment. The technical details
of the compositional model are different from \CFS's, and the
technical details of the pragmatic account are different from
\citeauthor{Grice75}, but we hope that it combines the best aspects of
both approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{A compositional lexical uncertainty model}\label{sec:model}

\marginnote{Added a reference to Ben Russell's thesis.}

We now present our mixed semantic--pragmatic model, which can be seen
as a conceptual fusion of the Montagovian semantic perspective in
\citet{Lewis70GS}, the signaling systems of \citet{Lewis69}, the
probabilistic rational speech acts perspective of
\citet{Frank:Goodman:2012} and \citet{Goodman:Stuhlmuller:2013}, the
iterated best response model of \citet{Jaeger:2007,Jaeger:2011} and
\citet{Franke09DISS}, and the Bayesian view of Gricean reasoning
developed by \citet{Russell:2012}.  Our Python implementation of the
model is available from the website for this paper.

The model we implement here is a direct extension of the compositional
lexical uncertainty model of \citet{Bergen:Goodman:Levy:2012} and
\citet{Bergen:Levy:Goodman:2014} (see also
\citealp{lassiter-goodman:2013SALT,lassitergoodman15} for a closely related
variant). This model defines production and
interpretation as recursive processes in which speakers and listeners
reason jointly about the state of world and the precise interpretation
of lexical items in context.  Our extension simply allows for greater
diversity in the semantic lexicon and includes more complex aspects of
semantic composition. Thus, in many ways, our central theoretical
result is that \citeauthor{Bergen:Levy:Goodman:2014}'s model predicts
embedded implicatures in non-monotone and downward-monotone contexts
if it is combined with a full theory of semantic composition.

\marginnote{Some refs.\ on the reality of lexical uncertainty. More would be great!}

The model's crucial feature is \tech{lexical uncertainty}.  In
semantics, we like to imagine that word meanings are fixed across
speakers and contexts, but in fact they are sometimes idiosyncratic
and usually adaptable \citep{clark-clark79,Clark97}. Thus, in our model, discourse
participants are not presumed to share a single, fixed
lexicon. Rather, they consider many lexica, and their communicative
behavior, in both production and interpretation, is guided by their
best attempts to synthesize the information from these varied sources
\citep{Giles:Coupland:Coupland:1991}. Thus, in the sentences of
interest, the discourse participants might entertain multiple senses
for an embedded \word{some}, including not only its `at least' meaning
but also the `only some' meaning that corresponds to its enrichment by
scalar implicature. This uncertainty carries through the compositional
semantics to deliver embedded implicature readings. From this
perspective, \citeauthor{ChierchiaFoxSpector08}'s model is
conceptually very close to lexical uncertainty, in that it requires
reasoning about the logical form that a speaker intends to convey; a
given token of \word{some} can take on multiple senses depending on
the presence and nature of silent embedded operators in the logical
form. Our extension of \citeauthor{Bergen:Levy:Goodman:2014}'s model
shows how this uncertainty guides pragmatic reasoning, and it
furthermore shows that the uncertainty need not be fully resolved in
order for robust pragmatic inferences to go through.

% As we said above, the model is a minor extension of the one
% presented in detail in \citet{Bergen:Levy:Goodman:2014}, which
% builds on the presentation of \citet{Bergen:Goodman:Levy:2012}. Our
% technical contribution here is to define an expanded view of lexical
% uncertainty and to study the effects this expansion has in a fully
% compositional intensional logic with quantifiers. Our Python
% implementation of the model is available from the website for this
% paper.

%=====================================================================

\subsection{Grammar fragment}\label{sec:grammar}

\Tabref{tab:grammar} gives the intensional fragment that we use
throughout the remainder of this paper, both to explain how our
pragmatic model works and to conduct our experimental analyses in
\secref{sec:binary}. It is our base lexicon, subject to refinement as
part of pragmatic inference.

The formal presentation is influenced by that of \citet{Muskens95}:
all of the denotations are sets, and the rules of semantic composition
(the final four lines) combine them using operations that are formally
akin to functional application. Our motivation for this less familiar
presentation is that it makes it easy to define a uniform notion of
refinement throughout the lexicon.

\begin{table}[t]
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}[c]{r@{ $\rightarrow$ }l l}
    \toprule
    \multicolumn{2}{c}{Syntax}     & Denotation of the lefthand side of the syntax rule\\
    \midrule
    N   & \word{person}      & $\set{\tuple{w, x} : x \text{ is a person in } w}$ \\
    N   & \word{shot}        & $\set{\tuple{w, x} : x \text{ is a shot in } w}$ \\
    \Vt & \word{hit}         & $\set{\tuple{w, x, y} : x \text{ hit } y \text{ in } w}$ \\
    \Vi & \word{scored}      & $\set{\tuple{w, x} : \exists y \ x \text{ hit } y \text{ in } w}$ \\
    \Vi & \word{cheered}     & $\set{\tuple{w, x} : x \text{ cheered in } w}$ \\
    D   & \word{some}        & \genericquantifier{\cap}{\neq \emptyset} \\
    D   & \word{every}       & \genericquantifier{\subseteq}{} \\
    D   & \word{no}          & \genericquantifier{\cap}{= \emptyset} \\
    D   & \word{exactly one} & \genericquantifier[cardinality]{\cap}{= 1} \\
    NP  & \word{Player A}     & \genericpn{\playera} \\
    NP  & \word{Player B}     & \genericpn{\playerb} \\
    NP  & \word{Player C}     & \genericpn{\playerc} 
    \\[1ex]    
    NP  & D N         & $\set{\tuple{w, Y} : \tuple{w, \gsem{N}, Y} \in \gsem{D}}$ \\
    VP  & \Vt\ NP     & $\set{\tuple{w, x} :  \set{\tuple{w, y} :  \tuple{w, x, y} \in \gsem{\Vt}} \in \gsem{NP}}$ \\
    VP  & \Vi         & $\gsem{\Vi}$ \\
    S   & NP VP       & $\set{w : \tuple{w, \gsem{VP}} \in \gsem{NP}}$ \\
    \bottomrule
  \end{tabular}
  \caption{Interpreted grammar fragment. The left column defines a context-free grammar,
    and the right column gives its recursive interpretation in an intensional model
    $\tuple{\Domain, \Worlds, \sem{\cdot}}$, where $\Domain$ is a set of entities,
    $\Worlds$ is a set of possible worlds, and $\sem{\cdot}$ is a semantic interpretation
    function. Notational conventions: $x, y \in \Domain$, $w \in \Worlds$, and $X, Y \subseteq (\Worlds \times \Domain)$.}
  \label{tab:grammar}
\end{table}

%=====================================================================

\subsection{Refinement}\label{sec:refine}

\marginnote{More discussion of the prevalence of lexical uncertainty, with new examples.}

The grammar in \tabref{tab:grammar} contains both lexical entries and
rules of semantic combination. We assume that the rules are fixed.
The lexical entries, on the other hand, are merely a starting point
for linguistic communication --- a set of somewhat negotiable
conventions. You might assume that \word{couch} and \word{sofa} are
synonymous, but if I say ``It's a couch but not a sofa'', you'll learn
something about my lexical representations and perhaps adjust your own
accordingly for the purposes of our interaction.  If a speaker uses
the phrase \word{synagogues and other churches}, then the listener can
conclude that she regards a synagogue to be a kind of church, via the
presuppositional nature of the phrase. Conversely, if she says
\word{church or synagogue}, the listener receives a weak signal that
she regards those two terms as disjoint, via the pressure for
disjuncts to be exclusive;
\citep{Hurford:1974}. \citet{Chemla-HurfordCounts} and
\citet{Potts:Levy:2015} connect this uncertainty directly with
implicature salience and implicature blocking.

The `lexical uncertainty' aspects of our model are designed to capture
this variability. The core notion is that of lexical
\tech{refinement}, as defined in \eg{refinement}.
%
\begin{examples}
\item\label{refinement} 
  \begin{examples}
  \item Let $\varphi$ be a set-denoting expression. $\refine$ is a
    \tech{refinement} of $\varphi$ iff $\refine \neq \emptyset$ and
    $\refine \subseteq \sem{\varphi}$.
  \item\label{refine} $\Refine(\varphi)$, the set of refinements for
    $\varphi$ in context $c$, is constrained so that
    $\sem{\varphi} \in \Refine(\varphi)$ and
    $\Refine(\varphi) \subseteq \wp(\sem{\varphi}){-}\emptyset$
  \end{examples}
\end{examples}
%
The full possible refinement space for a lexical item is the power set
of its denotation minus the empty set. In a functional presentation of
the interpreted fragment, this could instead be defined in terms of
the subfunctions of a given denotation using a cross-categorical
notion of entailment. With \subeg{refinement}{refine}, we allow that
contexts can vary in how much of the full refinement space they
utilize. They can be as small as the original denotation (in which
case the uncertainty is eliminated), or as large as the full power set
(minus the empty set).

\marginnote{I was tempted to finish this para by saying people don't literally reason in terms of full lexica, but that this is a good model of their behavior. R1 raised this concern. I stopped short because I thought it might confuse people or raise a concern they don't actually have.}

The guiding idea is that, in interaction, pragmatic agents reason
about possible refinements of their lexical items, with the base
lexical meaning serving as a kind of anchor to which each word's
interpretation is loosely tethered.  Intuitively, one can imagine that
part of what it means to be a responsible interlocutor is to make
inferences, based on the speaker's behavior, not only about the world
information she would like to convey, but also about the precise
meanings she intends the words she is using to carry in the context of
the interaction.

\marginnote{Corrected minor errors in this paragraph noted by R1 and R1. By the way, this is paragraph the editor asked about, but I can't see what the concern is.}

As we noted above, \CFS's model embodies a kind of semantic
uncertainty very similar to that considered here. For any given
expression that one hears, the speaker might have in mind its literal
content $\sem{\varphi}$ or one of the many enrichments available with
$\OALT(\varphi)$ for different choices of $\ALT$. Similarly, we admit
the trivial refinement $R = \sem{\varphi}$ as well as enrichments
(technically, subsets) of it. The major technical difference lies in
how these sets of denotations enter into the compositional
semantics. For \CFS, the alternatives all contribute to a single
denotation, whereas our model keeps the alternatives separate during
semantic composition, synthesizing them only for pragmatic
inference. In terms of \figref{fig:qspace}, we saw that \CFS's theory
uses $\OALT$ to create a single refined meaning for \word{some shot},
represented by the set of sets in the gray box (`some, not all'). Our
theory of refinement could create one lexicon for every non-empty
subset of the green box. So, in addition to considering `some, not
all, shots, we admit lexica that produce
$\sem{\word{some shot}} = \set{\set{a,b,c}}$ (`every player'), lexica
that produce $\sem{\word{some shot}} = \set{\set{a,b,c},\set{a}}$
(no obvious paraphrase), and so forth. These are all potential results
of $\OALT(\word{some shot})$ for some choice of $\ALT$, and our theory
can be regarded as one that reasons in terms of all of these options.

%=====================================================================

\subsection{Pragmatic reasoning}\label{sec:agents}

Our pragmatic model combines the logical grammar of
\secref{sec:grammar} with the lexical refinements of
\secref{sec:refine}. The basic ingredients are given in
\eg{modobjects}. We take as given a context $c$, an interpreted
fragment $\tuple{\Grammar, \Domain, \Worlds, \sem{\cdot}}$ as in
\tabref{tab:grammar}, with context free grammar $\Grammar$, a domain
of entities $\Domain$, a set of worlds $\Worlds$, an interpretation
function $\sem{\cdot}$ interpreting expressions of $\Grammar$ in these
domains, and a refinement function $\Refine(\varphi)$ that is defined
for all lexical items in $\Grammar$.
%
\begin{examples}
\item\label{modobjects}
  \begin{examples}
  \item\label{messages} $\Messages$ is a subset of the
    proposition-denoting expressions generated by $\Grammar$. It is
    augmented with a null message $\nullmsg$ such that
    $\sem{\nullmsg} = \Worlds$.

  \item\label{lexset}% 
    $\LexSet = \set{\Lex' : \Lex'(\nullmsg) = \Worlds \text{ and } \Lex'(\msg) \in \Refine(\msg)}$             
  
  \item $\StatePrior : \Worlds \mapsto [0,1]$ is a prior probability
    distribution over worlds.

  \item $\Costs : \Messages \mapsto \Reals$ is a cost function on
    messages.  For lexical items, costs are specified. For a
    nonterminal node $A$ with daughters $B_{1} \ldots B_{n}$,
    $\Costs(A) = \sum_{i=1}^{n}\Costs(B_{i})$.

  \item $\LexPrior : \LexSet \mapsto [0,1]$ is a prior probability
    distribution over lexica.
  \end{examples}
\end{examples}

In this paper, we do not bias the prior distribution over states
$\StatePrior$ or the prior distribution over lexica $\LexPrior$ in any
way, assuming them to be flat. Since we do not have experimental
measurements for the priors, this seems like the safest option. (For
techniques for measuring and manipulating state priors, see
\citealt{Frank:Goodman:2012} and
\citealt{Stiller:Goodman:Frank:2011}.)  Similarly, we do not explore
different cost functions on messages, assuming them all to be $0$ for
simplicity. Our cost functions play a role only in disfavoring the
`null message' $\nullmsg$, which is stipulated to be true in all
worlds in all lexica.

\marginnote{This paragraph is new, responding to R1's queries about the null message.} 

The null message gathers together all of the messages that are
unusable in the context because they don't make any relevant
distinctions, so the speaker remains silent if forced into this
situation. At a technical level, $\nullmsg$ allows us to explore the
full space of refinements for messages without encountering a
situation in which all the available messages have $0$ probability
given the speaker's observed state, which would result in
undefinedness. There are a few alternative methods for addressing this
technical issue. \citet{Bergen:Goodman:Levy:2012} admit only lexica in
which the speaker has at least one true message for every state;
\citet{Bergen:Levy:Goodman:2014} briefly consider giving false states
tiny positive probability; and \citet{Jaeger:2011} defines a
belief-revision step to handle comparable situations in the context of
the iterated best-response model. We favor positing the null message
because it corresponds to a clear linguistic intuition (the speaker
might sometimes choose to remain silent), and it minimally impacts how
the listener construes the other messages.  We set
$\Costs(\nullmsg) = 5$ throughout the paper, but changing this value
does not change our qualitative predictions.
%(\Appendixref{app:paramexplore} explores alternative settings.)

Our focus is on the space of lexica defined by
\subeg{modobjects}{lexset} given a certain set of relevant messages,
as in \subeg{modobjects}{messages}. Clause~\subeg{modobjects}{lexset}
specifies all of the possible lexica given the original interpretation
function $\sem{\cdot}$ and $\Refine$. It is the space opened up by
these constructs that allows us to predict where and how embedded
implicatures will be perceived as salient. It should be noted in this
context that our decision to refine only lexical items, as in
\subeg{modobjects}{lexset}, is made only for simplicity. We could
allow arbitrary words and phrases to be refined, as \CFS\ in effect
do.

\marginnote{Here and in the following paragraphs, I've tried to more fully explicate the definitions.}

With this background in place, we now define the core lexical
uncertainty model. It consists of three inter-related agents, as
defined in \eg{agents}. The agents are defined in terms of the cost
function $\Costs$, the state prior $\StatePrior$, and the lexica in
$\LexSet$. We assume throughout that $\msg$ is any message in
$\Messages$, $\state$ is any state in $\States$, and $\Lex$ is any
lexicon in the set $\LexSet$.\footnote{$P(a \given b) \propto X$ is
  read `the value $P(a \given b)$ is proportional to the value
  $X$'. In this context, the exact value of $P(a \given b)$ can always
  be obtained by dividing $P(a \given b)$ by the sum of all the values
  $X'$ obtained by replacing $a$ by one of its $a'$.}
%
\begin{examples}
\item\label{agents}
  \begin{examples}
    \marginnote{I took up R3's suggestion about how to specify $\listenerZero$. To me, the new version is more gnomic, but I think it does force a deeper understanding of the use of $\propto$ throughout.}

  \item\label{l0}%
    $\listenerZero(\state \given \msg, \Lex) \propto \StatePrior(\state) \text{ if } \state \in \Lex(\msg), \text{ else } 0$

% would this be clearer if we just wrote it as a conditional probability? something like
% 	$\listenerZero(\actualstate = \state \given \msg, \Lex) = \StatePrior(\state \given \actualstate \in \Lex(\msg))$
% the cost would be adding into the model a notion of 'actual state', which is merely implicit at present.
% this might also allow us to simplify the prose, as in comment added below.

  \item\label{s1}% 
    $\speakerOne(\msg \given \state, \Lex) \propto
    \exp
    \left(
      \log\listenerZero(\state \given \msg, \Lex)
      - 
      \Costs(\msg)
    \right)$
    
  \item\label{L} 
    $\UncertaintyListener(\state \given \msg) 
    \propto 
    \StatePrior(\state)
    \sum_{\Lex \in \LexSet}
    \LexPrior(\Lex)
    \speakerOne(\msg \given \state, \Lex)$
  \end{examples}
\end{examples}

The first two agents, $\listenerZero$ and $\speakerOne$, are
fixed-lexicon agents, and the final listener $\UncertaintyListener$
reasons over all of the lexica in $\LexSet$.  The most basic agent is
$\listenerZero$. It defines a conditional distribution over worlds
$\state$ given messages $\msg$. It does this by simply mapping the
truth conditions into a probability distribution and incorporating the
state prior. So this is just the semantics turned into a probability
distribution for the sake of decision making; the intuitive idea is
that agent heard $\msg$ and estimates the relative likelihood of world
$\state$ on that basis.

% Possible revised gloss, to go along with suggested revision above:
	% ``$l_0$ simply conditions her prior distribution on the truth of the literal interpretation, relative to the given lexicon''

The speaker agent $\speakerOne$ is already a pragmatic agent, in the
sense that it reasons, not about the lexicon directly, but rather
about how the listener will reason about the lexicon. The speaker
observes a state $\state$ and chooses messages on that basis. The
logarithm and exponentiation in this definition allow us to include
real-valued costs; where the costs are all $0$, it reduces to
$\listenerZero(\state \given \msg)$, by the identity
$x = \exp(\log(x))$.  Similarly, if the costs are scaled into $[0,1]$,
then the values can be given as a product of the listener
probabilities and the costs, analogously to the listener.

Our pragmatic listener is defined in \subeg{agents}{L}. This agent
resembles the literal listener $\listenerZero$, but it sums over all
of the inferences defined by the lexicon-specific agents $\speakerOne$
and $\listenerZero$. It additionally incorporates the state prior, as
$\listenerZero$ does, and the prior over lexica.  This is the agent
that we use to characterize listener inferences and define our
predictions about our experimental findings.

\marginnote{This paragraph is heavily revised. It now introduces different parameter settings and introduces our appendix on parameter exploration.}

We have given the compositional lexical uncertainty model in its
simplest form, and have gone beyond
\citeauthor{Bergen:Levy:Goodman:2014}\ only in giving the more
complete treatment of semantic composition, in allowing uncertainty in
the denotations of lexical items of a wider range of semantic types,
and in entertaining the possibility of restrictions on the set of
possible refinements.  Many elaborations are possible, and these can
have far-reaching consequences for predictions about pragmatic
inference \citep{Goodman:Lassiter:2013, Smith:Goodman:Frank:2013,
  Kao-etal:2014,Potts:Levy:2015}. Ideally, we would explore the full
space of reasonable models. Since the nature of this space is unknown,
this is impossible. However, there are two elaborations of the above
model that are highly salient given the prior literature, and thus
they are worth exploring. First, one could allow further iteration
beyond $\UncertaintyListener$, defining speaker and listener agents
analogously to their fixed-lexicon counterparts. This can amplify
existing pragmatic inferences and create new ones
\citep{Bergen:Levy:Goodman:2014,Vogel-etal:2014,Potts:Levy:2015}.
Second, one can include a real-valued temperature parameter $\lambda$
in the speaker agents to control how greedily they try to extract
information from the agent they are reasoning about, with higher
$\lambda$ values leading to more aggressive inferential strategies
\citep{Sutton:Barto:1998}. This too can radically reshape the agents'
behavior.  In \appendixref{app:paramexplore}, we systematically
explore these choices and their interaction with the cost assigned to
the null message.

%=====================================================================

\subsection{Illustrations}\label{sec:illustrations}

Our first illustration, given in \figref{fig:simplescalar}, is
designed solely to reveal details about how the agents interact to
produce enriched construals. (This first illustration is isomorphic to
the example covered in section~4.4 of
\citealp{Bergen:Levy:Goodman:2014}.) We assume that the domain
consists of just one entity, $\playera$, and that the only intensional
distinction of interest is whether $\playera$ scored none of his shots
(world \world{N}), some but not all of his shots (\world{S}), or all
of his shots (\world{A}). The action is in the relationship between
the two predicates \word{scored} and \word{aced}: we define
$\sem{\word{scored\/}} = \set{\tuple{\world{S}, \playera},
  \tuple{\world{A}, \playera}}$
and $\sem{\word{aced\/}} = \set{\tuple{\world{A}, \playera}}$. Thus,
\word{aced} strictly entails \word{scored}, creating the potential for
a scalar implicature.

\begin{figure}[!t]
  \[
  \setlength{\arraycolsep}{5pt}
  %\renewcommand{\arraystretch}{0.95}
  \begin{array}{r c c c}
    \UncertaintyListener &
    & \genericscalar{0}{\graycell{.71}}{.29}{0}{0}{\graycell{1}}{.75}{.25}{0}
    \\
    & \multicolumn{1}{r}{\swarrow}& \multicolumn{1}{c}{\downarrow}& \multicolumn{1}{l}{\searrow}
    \\
    \speakerOne
    &
    \scalarspeaker{0}{0}{1}{.99}{0}{.01}{.33}{.67}{0}
    &
    \scalarspeaker{0}{0}{1}{.99}{0}{.01}{0}{.99}{.01}
    &
    \scalarspeaker{0}{0}{1}{0}{0}{1}{.5}{.5}{0}
    \\
    & \downarrow & \downarrow & \downarrow
    \\
    \listenerZero
    &
    \genericscalar{0}{.5}{.5}{0}{0}{1}{.33}{.33}{.33}
    &
    \genericscalar{0}{1}{0}{0}{0}{1}{.33}{.33}{.33}
    &
    \genericscalar{0}{0}{1}{0}{0}{1}{.33}{.33}{.33}
    \\
    & \downarrow & \downarrow & \downarrow 
    \\    
    \Messages
    &
    \genericscalar{\False}{\True}{\True}{\False}{\False}{\True}{\True}{\True}{\True}
    &
    \genericscalar{\False}{\True}{\False}{\False}{\False}{\True}{\True}{\True}{\True}
    &
    \genericscalar{\False}{\False}{\True}{\False}{\False}{\True}{\True}{\True}{\True}
    \\
    & \uparrow & \uparrow & \uparrow 
    \\                               
    \Lex
    & 
    \scalarlex{\tuple{\world{S},\playera}, \tuple{\world{A},\playera}}
    & 
    \scalarlex{\tuple{\world{S},\playera}}
    &
    \scalarlex{\tuple{\world{A},\playera}}   
  \end{array}
  \]
  \caption{Simple scalar inference. 
    We assume a flat prior over states and lexica. 
    $\Costs(\nullmsg) = 5$, and $\Costs(\msg) = 0$ for the other messages. 
    The uncertainty listener $\UncertaintyListener$ infers that the general term 
    \word{scored} excludes its specific counterpart \word{aced} in this context.}
  \label{fig:simplescalar}
\end{figure}

\marginnote{Added a note, at the end of the paragraph, about what that probability means. Perhaps this needs its own paragraph, but I am wary of too much methodological hand-wringing --- such prose can be the hardest of all on readers.}

To keep the example compact, we let
$\Refine(\word{Player A}) = \set{\sem{\word{Player A}}}$. Since
\word{aced} already denotes a singleton set, it has no space for
further refinement. However, \word{scored} has two further
refinements. This gives rise to the three lexica in the bottom row of
\figref{fig:simplescalar}.  Using the fixed rules of semantic
composition, these lexica determine the messages \word{Player A
  scored} and \word{Player A aced}. The literal listener
$\listenerZero$ turns the denotations of these messages into
conditional distributions over states given messages. The prior over
states is flat in this example, so this calculation just evenly
divides the probability mass over the true states. The pragmatic
speaker responds to this agent. Finally, our uncertainty listener sums
over these three speakers. This listener achieves a scalar implicature
in the following nuanced, probabilistic sense
\seccitep{Russell:2012}{2}. Hearing \word{Player~A scored} leads this
listener to assume that the most probable state is \world{S}. The
probability is not $1$, so uncertainty remains. However, if this
listener is compelled to make a categorical decision about the
intended meaning of the utterance, it will choose this enriched
construal, and it will rightfully feel deceived if the world state
turns out to be \world{A} or (worse) \world{N} instead. In this way,
the model characterizes the uncertainty surrounding implicature
inferences \citep{Hirschberg85} and the ways in which this uncertainty
relates to decision making.

Lexical uncertainty is not required to achieve this result. If we
allow no meanings to be refined, then we deal with the singleton set
of lexica containing only the leftmost lexicon. In this small space,
the model shares deep affinities with the Bayesian model of Gricean reasoning
given by \citet{Russell:2012}; it is effectively equivalent to the
rational speech act model of \citet{Frank:Goodman:2012} (with
potentially small differences relating to how the prior over states is
incorporated); and it can be seen as a more thoroughly probabilistic
version of the iterated best response model
\citep{Franke09DISS,Jaeger:2007,Jaeger:2011}. Nonetheless, the example
illuminates how the lexical uncertainty model works. As the downward
arrows indicate, it is useful to start conceptually from
$\UncertaintyListener$. This agent effectively reasons in Gricean
terms about three separate lexica; the alternation from speaker to
listener and down to the lexicon mirrors the nested belief structure
of Grice's original definition of implicature (sketched at the start
of \secref{sec:implicature}).

Even though we assume an even prior over lexica, useful biases emerge
because the space of lexica is structured: there are no lexica in
which \word{aced} is consistent with \world{S}, but there are two in
which \word{scored} is. This bias carries through the computation to
create a strong final bias for the implicature inference. For further
discussion of this important point, we refer to
\citealt{Bergen:Levy:Goodman:2014}, where it is shown to be essential to
generating implicatures based on the principle that marked forms
signal marked meanings and unmarked forms signal unmarked meanings
\citep{McCawley78,Horn84,Blutner98,Levinson00}.

\marginnote{This para and the next are new, as is \tabref{tab:sauerland}. They seel to answer R3's concerns about ``global'' implicature.}

The lexical uncertainty aspects of the model are a rich source of
implicatures, and they are the key to achieving local implicatures of
the sort reviewed in \secref{sec:implicature} above. However, as the
fixed lexicon versions of the model make clear, the recursive nature
of the agents suffices for many kinds of enrichment. Even with a
single lexicon, we have a listener reasoning about a speaker reasoning
about the literal interpretive semantics, which creates powerful
forces for removing semantic overlap among the available messages. One
powerful illustration of this comes from
\citet{Sauerland01,Sauerland04}, who studies the implicatures of
sentences like \word{Player~A hit some of his shots or cheered}, in
which the weak scalar term \word{some of his shots} is nested inside
the weak connective \word{or}. The guiding intuition is that the
sentence is most prominently construed as entailing that Player~A did
not make all of his shots and that Player~A did not both make shots
and cheer. \citeauthor{Sauerland01}'s insight is that these
entailments are within reach of traditional neo-Gricean reasoning as
long as the available alternative messages that the speaker might have
used is comprehensive in that it fully crosses the alternatives for
\word{some} with the alternatives for \word{or}.

As \tabref{tab:sauerland} shows, our model suffices to achieve this
even with a fixed lexicon. Here, a world-type specification like
\world{s$_{1}$c} means that player A made only shot~1 and cheered,
\world{s$_{1}$s$_{2}$c} means that Player~A made both shot~1 and
shot~2 and cheered, \word{c} means that he made no shots but did
cheer, and the dash identifies the world-type in which Player~A made
no shot and did not cheer. The crucial analytic step is to define the
set of messages $\Messages$ so that it covers the space that
\citeauthor{Sauerland01}. This suffices to capture the desired
inferences in the probabilistic sense that our model provides.
Allowing lexical refinements, as in the full version of our model,
strengthens the relevant inferences without changing the qualitative
pattern seen in \tabref{tab:sauerland}.\footnote{There is one
  unnatural aspect of the simulation in \tabref{tab:sauerland}: it
  does not allow the speaker to be uncertain about the state space,
  whereas disjunction is naturally used in situations where the
  speaker has only partial information even at the level of
  granularity given by the world-types in $\States$. This results in
  predicted overlap between \word{Player~A hit some shot or cheered}
  and \word{Player~A hit some shot}. For a version of the model that
  allows for the requisite sort of uncertainty by closing the state
  space under joins, we refer to \citealt{Bergen:Levy:Goodman:2014}
  and \citealt{Potts:Levy:2015}.}
  
\begin{table}[tp]
  \centering
  \renewcommand{\arraystretch}{1.05}
  \setlength{\tabcolsep}{8pt}
  \begin{tabular}[c]{l *{8}{r} }
    \toprule
    & \world{--} & \world{c} & \world{s$_{1}$} & \world{s$_{2}$} & \world{s$_{1}$c} & \world{s$_{2}$c} & \world{s$_{1}$s$_{2}$} & \world{s$_{1}$s$_{2}$c}\\
    \midrule
    \word{Player~A cheered}                    &    0 &   \graycell{.42} &    0 &    0 &   .23 &   .23 &    0 &   .12\\
    \word{Player~A hit some shot}               &    0 &    0 &    \graycell{.3} &    \graycell{.3} &   09 &   09 &   .18 &   04\\
    \word{Player~A hit some shot or cheered}   &    0 &   .14 &   \graycell{.26} &   \graycell{.26} &   07 &   07 &   .16 &   04\\
    \word{Player~A hit some shot and cheered}   &    0 &    0 &    0 &    0 &    \graycell{.4} &    \graycell{.4} &    0 &   .21\\
    \word{Player~A hit every shot or cheered}   &    0 &   .28 &    0 &    0 &   .15 &   .15 &   \graycell{.33} &   08\\
    \word{Player~A hit every shot and cheered} &    0 &    0 &    0 &    0 &    0 &    0 &    0 &    \graycell{1} \\
    $\nullmsg$ &   \graycell{1} &    0 &    0 &    0 &    0 &    0 &    0 &    0\\
    \bottomrule
  \end{tabular}
  \caption{Nested scalar terms. We assume that there are just two
    shots. Thus, \world{s$_{1}$s$_{2}$} is a world in which Player~A
    made every shot. \world{c} is a world in which Player~A made no
    shots, but cheered. And so forth.  The message of greatest
    interest is \word{Player~A hit some shot or cheered}. Consistent
    with intuitions reported in the literature, the model predicts
    that the highest probability inferences are those in which the
    player did not make all shots and did not both make shots and
    cheered. This inference derives solely from competitions between
    available messages. Lexical uncertainty only strengthens the basic
    pattern.}\label{tab:sauerland}
\end{table}

Let's now look at a larger and more complex scenario, one that
interacts lexical uncertainty with message competitions like the above
to help reveal the potential of this model to predict embedded
implicatures. In this scenario, there are two players. We continue our
convention of referring to worlds using sequences like \world{NN}
(`neither player scored'). The lexical items are \word{Player A},
\word{Player B}, \word{some}, \word{every}, \word{no}, \word{scored},
and \word{aced}.  To start, we assume that, for all lexical items
$\varphi$, $\Refine(\varphi) = \wp(\sem{\varphi}){-}\emptyset$.  This
creates an enormous space of lexica, and allows the full range of
possible interactions between the refinements.

The listener inferences are summarized in \tabref{tab:subjects}. For
the most part, they seem aligned with the general view in the
literature about how scalar terms interact in contexts like this. For
instance, we predict that a proper name \word{P} will take on the
exhaustified sense \word{only P}, as we would expect given the
salience of \word{every}. In turn, \word{some} is interpreted as
non-specific in virtue of the salience of the two names, and it also
leads to a scalar implicature due to the salience of
\word{every}. Perhaps the most striking outcome is that the scalar
inference from \word{scored} to not-aced remains in evidence not just
with the proper names but also in the scope of the quantified
subjects: the best-guess inference for \word{every player scored} is
\world{SS}.  These effects derive from interacting lexical uncertainty
between the subjects and predicates.

\Tabref{tab:subjects} reveals some drawbacks to unfettered exploration
of refinements, however. First, we might expect hearing \word{some
  player scored} to lead the listener to assume that the state was
either \world{NS} or \world{SN}, corresponding to enrichment of both
the subject (`not all players') and the predicate (`merely
scored'). The current model does not achieve this. In addition, the
row for \word{no player scored} is unintuitive. The best inference is
\world{NN}, which is in line with the literal semantics, but it is
striking that the states \world{NS} and \world{SN} have some positive
probability. This arises because of interacting lexical uncertainty:
there are lexica in the space in which \word{scored} is refined to
exclude one of the players. In that case, the negative universal turns
out to be true. Only a few lexica support this interaction, ensuring
that it cannot become dominant, but it still seems worrisome.

This worry is a touchstone for revisiting an
assumption of the model underlying \tabref{tab:subjects}: that the lexical
items can be refined in completely arbitrary ways. We take it to be one of the
major lessons of neo-Gricean approaches that alternatives are
contextually and lexically constrained. \CFS's treatment of $\OALT$
reflects this insight, and our own handling of refinement and lexical
uncertainty allows us to incorporate it as well. So it is worth seeing
whether we can improve the picture in \tabref{tab:subjects} by
encoding lexical scales in our grammar fragment.  We
implement this by constraining the refinement sets for
several lexical items, as follows:%
\footnote{We define $\sem{\word{only Player A}} = \set{\tuple{w, Y} : \set{\playera} = \set{x : \tuple{w,x} \in Y}}$,
  and similarly for $\sem{\word{only Player B}}$, not as a claim about natural language \word{only}, but rather just 
  for the sake of the simulation.}
%
\begin{examples}
\item\label{neo}
  \begin{examples}
  \item $\Refine(\word{Player A}) = \set{\sem{\word{Player A}}, \sem{\word{only Player A}}}$
  \item $\Refine(\word{Player B}) = \set{\sem{\word{Player B}}, \sem{\word{only Player B}}}$
  \item $\Refine(\word{some}) = \set{\sem{\word{some}}, \sem{\word{some and not all}}}$
  \item $\Refine(\word{no}) = \set{\sem{\word{no}}}$    
  \item $\Refine(\word{scored}) = \set{\sem{\word{scored}}, \sem{\word{scored and didn't ace}}}$
  \item $\Refine(\word{aced}) = \set{\sem{\word{aced}}}$
  \end{examples}
\end{examples}
%
The results of working in this more refined refinement space are given
in \tabref{tab:subjects-ALTstyle}. The picture is mostly unchanged,
except we now also achieve the target enrichment for \word{some player
  scored}, and the messiness surrounding \word{no player scored} is
fully addressed. The one remaining potential concern about
\tabref{tab:subjects-ALTstyle} is that it predicts rather aggressive
pragmatic enrichment of the scalar term in the scope of the negative
quantifier. As we noted in \secref{sec:implicature}, it has long been
assumed that scalar items in such environments fail to give rise to
implicatures. \citet{Chemla:Spector:2011} address this question
empirically, finding in their experiment low but non-negligible rates
of local enrichment in negative environments. We too treat this as an
empirical question; in \secref{sec:binary}, we present evidence that
local enrichments of this sort are indeed salient possibilities for
humans.

\begin{table}[t]
  \centering
  \renewcommand{\arraystretch}{0.98}
  \setlength{\tabcolsep}{8pt}
  %
  % Lexica: 2,086,465
  %
  \begin{tabular}[c]{l *{9}{r} }
    \toprule
    & \world{NN} & \world{NS} & \world{NA} & \world{SN} & \world{SS} & \world{SA} & \world{AN} & \world{AS} & \world{AA}\\
    \midrule
    \word{Player A scored}     & 0 & 0 & 0 & \graycell{.24} & .19 & .16 & .18 & .16 & .07\\
    \word{Player A aced}       & 0 & 0 & 0 & 0 & 0 & 0 & \graycell{.36} & .30 & .34\\
    \word{Player B scored}     & 0 & \graycell{.24} & .18 & 0 & .19 & .16 & 0 & .16 & .07\\
    \word{Player B aced}       & 0 & 0 & \graycell{.36} & 0 & 0 & .30 & 0 & 0 & .34\\
    \word{some player scored}  & 0 & .14 & .11 & .14 & \graycell{.17} & .14 & .11 & .14 & .05\\
    \word{some player aced}    & 0 & 0 & \graycell{.22} & 0 & 0 & .19 & \graycell{.22} & .19 & .18\\
    \word{every player scored} & 0 & 0 & 0 & 0 & \graycell{.31} & .27 & 0 & .27 & .14\\
    \word{every player aced}   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \graycell{1}\\
    \word{no player scored}    & \graycell{.31} & .14 & .12 & .14 & .06 & .05 & .12 & .05 & .01\\
    \word{no player aced}      & .18 & \graycell{.19} & .08 & \graycell{.19} & .14 & .06 & .08 & .06 & 0\\
    $\nullmsg$                 & .01 & .01 & \graycell{.32} & .01 & .01 & .15 & \graycell{.32} & .15 & 0\\
    \bottomrule
  \end{tabular}
  \caption{Enrichment in the largest space of refinements supported by this lexicon.}
  \label{tab:subjects}
\end{table}

\begin{table}[t]
  \centering
  \renewcommand{\arraystretch}{0.98}
  \setlength{\tabcolsep}{8pt}
  %
  % Lexica: 476
  %
  \begin{tabular}[c]{l *{9}{r} }
  \toprule
    & \world{NN} & \world{NS} & \world{NA} & \world{SN} & \world{SS} & \world{SA} & \world{AN} & \world{AS} & \world{AA}\\
    \midrule
    \word{Player A scored}     & 0 & 0 & 0 & \graycell{.45} & .11 & .22 & .15 & .05 & .02\\
    \word{Player A aced}       & 0 & 0 & 0 & 0 & 0 & 0 & \graycell{.42} & .36 & .22\\
    \word{Player B scored}     & 0 & \graycell{.45} & .15 & 0 & .11 & .05 & 0 & .22 & .02\\
    \word{Player B aced}       & 0 & 0 & \graycell{.42} & 0 & 0 & .36 & 0 & 0 & .22\\
    \word{some player scored}  & 0 & \graycell{.25} & .09 & \graycell{.25} & .06 & .12 & .09 & .12 & .01\\
    \word{some player aced}    & 0 & 0 & \graycell{.24} & 0 & 0 & .21 & \graycell{.24} & .21 & .11\\
    \word{every player scored} & 0 & 0 & 0 & 0 & \graycell{.61} & .16 & 0 & .16 & .07\\
    \word{every player aced}   & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \graycell{1}\\
    \word{no player scored}    & \graycell{.61} & 0 & .16 & 0 & 0 & 0 & .16 & 0 & .06\\
    \word{no player aced}      & \graycell{.19} & .17 & .10 & .17 & .13 & .07 & .10 & .07 & 0\\
    $\nullmsg$                 & \graycell{.15} & .13 & .13 & .13 & .10 & .09 & .13 & .09 & .05\\
    \bottomrule
  \end{tabular}
  \caption{Enrichment using the lexically-driven (neo-Gricean) refinement sets in \eg{neo}.}
  \label{tab:subjects-ALTstyle}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Prior experimental work}\label{sec:lit}

\todo{This material was extracted from the previous section and greatly expanded. Essentially all the prose is new. It would be great if everyone could read through it.}

The illustrative examples in the previous section begin to show that
our compositional lexical uncertainty model naturally generates local
enrichments. Thus, the question of whether listeners actually make
such inferences is extremely pressing. We now begin to pursue this
question directly. The present section reviews the prior literature in
this area, which provides clear and fairly comprehensive guidance.

The pioneering paper in this area is
\citealt{Geurts:Pouscoulous:2009}. Their experiments 3 and 4 asked
participants to provide truth-value judgments for sentences
interpreted in abstract visual scenes consisting of shapes connected
by lines. The target sentences included weak scalar terms in upward,
downward, and non-monotone contexts that are comparable in relevant
respects to the examples reviewed in \secref{sec:implicature} above.
\citeauthor{Geurts:Pouscoulous:2009} found only negligible rates of
inferences consistent with local enrichment. These findings stimulated
a number of responses commenting on the prevalence of local enrichment
and its theoretical import \citep{Ippolito:2010,Sauerland:2010}. The
two responses that are most relevant for our purposes are those of
\citet{Clifton:Dube:2010} and \citet{Chemla:Spector:2011}.

\citet{Clifton:Dube:2010} argue that the experimental setting used by
\citeauthor{Geurts:Pouscoulous:2009} was prone to understating the
rate of implicatures, and they sought to address this with a design in
which participants chose, from a set of scenes, which was the better
match for the prompt sentence. In this setting, participants tended to
chose the scene consistent with local enrichment. The design is a
natural choice in the context of a pragmatic model like ours, since it
places participants in a role comparable to that of a listener
agent. However, the method was criticized by
\seccitet{geurts-vantiel:2013:scalar}{5.1} and \citet{vanTiel:2014} on
the grounds that, for the examples involving monotone quantifiers, the
inferences are better explained in terms of the typicality effects of
the quantifiers involved (see also \citealt{Degen:Tanenhaus:2014}).
Roughly speaking, the claim is that the typicality structure of
\word{some A are B} favors situations in which just shy of half the
A's are B's, and experimental designs (like
\citeauthor{Clifton:Dube:2010}'s) that allow participants to express
extra-truth-conditional preferences will be sensitive to this
typicality structure. While we think that typicality is an important
component of many implicatures,\footnote{\posscitet{Levinson00}
  I-implicatures involve inferences from a general term or statement
  to one of its salient or prototypical subkinds. In the context of a
  generalized theory of scalar (partial-order) inference like that of
  \citet{Hirschberg85}, this can be seen as a scalar inference guided
  by prior expectations.} we see the value in trying to neutralize its
effects in the context of studying local enrichment.

\citet{Chemla:Spector:2011} followed \citet{Geurts:Pouscoulous:2009}
in asking participants to interpret quantified sentences in abstract
geometric scenes, but they sought to simplify those scenes (see
\pgcitealt{geurts-vantiel:2013:scalar}{31} for criticisms of this
presumption), and they allowed subjects to provide graded truth-value
judgments on a scale between `Yes' and `No'. The results were
consistent with very high rates of local enrichment in upward and
non-monotone environments, and even yielded suggestive evidence for
local enrichment in downward monotone environments. These findings
stand in stark contrast to those of \citet{Geurts:Pouscoulous:2009}.

However, there are at least three features of the experimental design
that might have exaggerated the rates of judgments consistent with
local enrichment \citep{geurts-vantiel:2013:scalar}. First, the graded
response categories mean that, for the monotone cases, typicality
effects might have played a role. Second, the visual scenes were
wheel-like displays in which lines extend from the vertex to the
perimeter. There are potentially many ways this state can be drawn.
Some might be more iconic than others, and some might create spurious
patterns and salience contrasts that could affect linguistic inference
in unmeasured ways. Third, \citeauthor{Chemla:Spector:2011} used a
within-subjects design: the individual participants judged every
sentence in every context. This means that subjects could draw
comparisons across different conditions, which creates opportunities
for them to register comparative judgments involving the experimental
contexts itself, rather than relying solely on their linguistic
intuitions.

We draw three major lessons from the above studies and debates. First,
we should seek out simple, naturalistic stimuli. Previous experiments
in this area have used abstract displays. Together with the inevitable
complexity of the sentences involved, this seems likely to be
cognitively demanding in ways that could affect the stability and
reliability of the responses. Second, scalar response categories might
encourage typicality inferences that could cloud the implicature
picture; this might be a concern only for monotone environments, but
we can hope to avoid the issue by restricting to just truth-value
responses. Third, to the greatest extent possible, we should seek a
design that supports analyses in which we can marginalize out the
idiosyncrasies of particular displays, to avoid artifacts of salience
or contrast that could stimulate responses that are consistent with
implicature calculation without requiring such calculation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiment: Scalars under quantifiers}\label{sec:binary}

\marginnote{Revised framing to connect with the prior lit discussion.}

We now present our main experiment involving \word{some} in quantified
environments. We told participants that they were helping to train an
automated sportscasting system and asked them to provide truth-value
judgments about sentences in the context of displays like
\figref{fig:binary:materials}. This cover story was designed to
ensure that implicatures are relevant, that is, worth calculating
where available (\seccitealt{Chemla:Spector:2011}{3.1};
\citealt{Clifton:Dube:2010}). Our goal was to better understand the
extent to which certain pragmatic inferences are available, so we
sought out a scenario that would be maximally favorable to
them.\footnote{For studies aimed at understanding the prevalence of
  implicatures, see \citealt{Paris:1973,Hendriks-etal:2009}.}

%=====================================================================

\subsection{Methods}

\marginnote{Updates throughout this section for the sake of the new design.}

\subsubsection{Participants}

The experiment had 800 participants, all recruited with Amazon's
Mechanical Turk. No participants or responses were excluded.

\subsubsection{Materials}

\begin{figure}[t]
  \centering
  \framebox{\includegraphics[width=0.8\textwidth]{fig/experiment-display-binary}}
  \caption{Experiment display.}
  \label{fig:binary:materials}
\end{figure}

We generated displays like those in \figref{fig:binary:materials}. In
each display, each of the three players, A, B, and C, has taken 12
basketball shots (a number small enough for visual display but outside
of the subitizing range and thus less likely to introduce competitions
from cardinal determiners like \word{three shots};
\citealt{Degen:Tanenhaus:2014}). The shots were divided into two
piles, labeled `baskets' (green) and `misses' (red).  For our target
items, the player either made all 12 baskets (Player~A in
\figref{fig:binary:materials}), missed all 12 baskets (Player~B), or
made six and missed 6 (Player~C). The colors of the players clothes
were set randomly from a palette of 14 colors.

The target sentences describing the displays were defined as follows:
%
\begin{examples}
\item\label{expmsgs} 
  $\set{
      \begin{tabular}[c]{l}
        Every \\
        Exactly one \\
        No 
      \end{tabular}}$
    player hit 
    $\set{
      \begin{tabular}[c]{l}
        all \\
        none \\
        some 
      \end{tabular}}$
    of his shots.  
\end{examples}
%
Following previous studies, we put a bound pronoun in the embedded
quantifier to try to ensure that the subject took scope over the
object. The partitive forms seem likely to further encourage
implicature calculation
\citep{reed:1991-interpreting,Grodner-etal:2010,degen:inpress-SP}. We
chose the verb \word{hit} over the slightly less marked verb
\word{make} to try to avoid the sense of `make' as in `take'
(consistent with missing).

For the target items, there were nine different conditions,
corresponding to the worlds in \eg{conds}, in the notation we've been
using to identify possible worlds.
%
\begin{examples}
\item\label{conds} $\set{\world{NNN}, \world{NNS}, \world{NNA},
    \world{NSS}, \world{NSA}, \world{NAA}, \world{SSS}, \world{SSA},
    \world{SAA}, \world{AAA}}$
\end{examples}
%
\marginnote{I extended this paragraph with a bit more on the rationale behind all the randomization we did.}
%
This is a subset of the full cross-product of the three outcomes
\world{N}, \world{S}, and \world{A} in which player $i$ always did at
least as well as player $i+1$, going left to right.  Our target
sentences were all quantified, so we don't care about the outcome for
any single player, meaning that we don't distinguish, e.g.,
\world{NNS} from \world{NSN}, allowing us to work with this smaller
set of conditions. In the experiment, the `order' of each world was
randomized, so that, e.g., \world{NSA} appeared visually in each of
its three orders approximately the same number of times. This
randomization allows us to control for preferences in visual
processing that might naturally make on position or linear ordering of
player outcomes salient in unanticipated ways.

\subsubsection{Procedure} 

After reading our consent form, participants were given the following
cover story:
%
\begin{examples}
\item\label{coverstory}
  We are trying to train an automated sportscasting system to generate
  color commentary on simple competitions. We'd like you to make
  judgments about the quality of the comments it generates. We'll use
  these ratings to train our system further.
\end{examples}

After reading this cover story and some instructions, participants
were presented with three training items, designed to ensure that
participants understood the cover story, displays, and sentences. They
then judged 32 sentences, divided into nine target sentences and 23
fillers. The design was between-subjects: no experimental participant
judged the same sentence twice. The order of presentation of the items
was randomized.
 
Each sentence received a total of 800 responses. For the target
sentences, each sentence--world pair received between 58 and 103
responses (mean 80); this variation resulted from randomization in the
assignment of worlds to sentences.

Target sentences were presented below displays. Participants were
asked to evaluate sentences as either true or false. In this sense,
our participants acted as listeners who got to observe the speaker's
state and assess whether the speaker accurately described that state
with her utterance. We also conducted a variant of the experiment in
which participants gave responses on a seven-point Likert scale
ranging from `Bad description' to `Good description', to see whether
this would reveal information about the quality of the report. These
two versions of the experiment led to qualitatively identical
outcomes. \Appendixref{app:likert} reviews the details of this
experiment.

All the materials and response data for the experiment are available
at the website for this paper.

%=====================================================================

\subsection{Results}\label{sec:binary:results}

\Figref{fig:binary:results} summarizes the responses by target
sentence and the world in which it was evaluated. Overall,
participants made judgments that accurately reflected whether
sentences were true or false; accuracy was especially clear for the
sentences in the first two columns, which do not admit pragmatic
enrichment. For these cases, the responses were essentially
categorical. This pattern suggests that our method is appropriate for
measuring participants' interpretations.\footnote{The only exception
  to this general pattern is the sentence \word{No player hit none of
    his shots} (bottom middle). The percentage of `True' responses is
  lower than normal in all its true conditions and relatively high for
  \world{NNN}, where it is false on its literal construal. We
  hypothesize that this pattern reflects a negative concord construal,
  on which the embedded term is interpreted as equivalent to \word{any
    of his shots}, creating a meaning that is true only in
  \world{NNN}. Negative concord of this sort is productive in many
  dialects of English and understandable in all or nearly all of
  them. This likely created uncertainty about the intended meaning of
  the sentence, leading participants to disfavor it in general.}

\begin{figure}[!ht]
  \centering
  %\includegraphics[width=0.85\textwidth]{fig/basketball-pilot-2-11-14-results-parsed}
  \includegraphics[width=0.85\textwidth]{fig/basketball-binary-version-pilot-4-1-15-results-parsed}
  \caption{Mean ratings by sentence with bootstrapped 95\% confidence intervals.}
  \label{fig:binary:results}
\end{figure}

\marginnote{Revisions from here to the end of the section, just updating p-values and changing the prose to reflect the new response categories.}

We now turn to the critical conditions, reporting significance levels
for key theoretical comparisons based on the nonparametric
Mann--Whitney U test. Responses for \target{every}{some} (upper right)
were consistent with the hypothesis that \word{some} is locally
enriched in this condition. In particular, this sentence received the
greatest percentage of `True' responses in the \world{SSS} world. As
we reviewed in \secref{sec:implicature}, example \eg{everysome-sem},
in order to count as a complete report in this world, this sentence
requires either local enrichment or a Gricean calculation with
auxiliary premises. Worlds \world{SSA} and \world{SAA} received the
next highest percentages of `True' responses (lower than \world{SSS},
$p = 0.09$ and $p = 0.04$, respectively). Of all the literally true
worlds for this condition, \world{AAA} received the lowest percentage
of `True' responses (lower than \world{SSA} and \world{SAA}; both at
$p < 0.01$).  Only a simple Gricean calculation is required to account
for the higher rate of `True' for \world{SSA} and \world{SAA} compared
with \world{AAA}: in the latter world, the salient alternative
\word{every player hit all of his shots} is a more complete
description.

Nevertheless, \target{every}{some} is not a strong test of the
presence of local readings, since the entailment relations between the
readings introduce some indeterminacy into the analysis. In
particular, since the local enrichment entails the literal reading, we
can't be sure whether the `True' responses for \world{SSS} derive
entirely from the availability of a local enrichment: a literal
construal would suffice to make the sentence true. Furthermore, as
discussed in \secref{sec:implicature} and in
\citeauthor{Chemla:Spector:2011}, \target{every}{some} is of limited
utility in distinguishing theoretical proposals anyway. It is the
\target{exactly one}{some} sentence that allows us to probe most
confidently for local readings.

The response pattern for the critical item \target{exactly one}{some}
is given in the middle right of \figref{fig:binary:results}. The
highest percentage of `True' responses is for the \world{NNS}
condition, where it is true under its literal and local enrichment
construals. However, it was also frequently judged true in the
\world{NSA} and \world{SAA} worlds (both higher at $p < 0.001$ than in
\world{SSA}, the world yielding the highest rating among those in
which the sentence is false both literally and under all possible
enrichments). For \world{NSA} and \world{SAA}, the sentence is true
only with local enrichment (because two players hit at least some of
their baskets in these worlds, ruling out the literal construal).  We
note also that its more strictly truth-conditional interpretation
seems to be salient as well, as it was generally perceived to be true
in the \world{NNA} condition.

Finally, the pattern for \target{no}{some} also suggests a non-trivial
amount of local enrichment: though \world{NNN} produced the highest
rate of `True' responses, indicating a preference for a literal
construal, the `True' rates for \world{NNA}, \world{NAA}, and
\world{AAA} are consistently higher than for the most favored false
worlds, \world{NNS} and \world{NSA}; all pairwise significance tests
for the cross-product of $\set{\world{NNS}, \world{NSA}}$ and
$\set{\world{AAA}, \world{NNA}, \world{NAA}}$ are significant at
$p = 0.002$. These are the worlds in which no player hit only some of
his shots, the local enrichment. This finding seems consistent with
the low but non-negligible rates of local enrichment that
\seccitet{Chemla:Spector:2011}{4.4.4} report for this quantifier
pair. One qualification we should add here is that our sentence is
arguably somewhat unnatural in that it places \word{some}, a positive
polarity item \citep{Baker70,Israel96}, in the scope of a negative
quantifier. The binding relation between the subject and the pronoun
\word{his} in the embedded phrase should force a surface-scope
reading, but we can't rule out the possibility that participants might
have found an inverse-scope construal (`some shots are such that no
player hit them') that took the scalar term out of the scope of the
negation. Alternatively, the marked nature of \word{some} in this
position might have encouraged implicit prosodic focus, which would
also likely increase the `only some' construals.

We conclude from these responses that local enrichment is possible
even in non-monotone environments, and that local enrichment might be
available in downward-monotone environments as well. However, our
concern is not only whether such readings are possible or impossible,
but rather how accurately we can predict their availability on the
basis of contextual and world knowledge.  We turn now to the task of
assessing the ability of the model in \secref{sec:model} to match both
the quantitative and qualitative patterns in our experimental data.

%=====================================================================

\subsection{Model assessment}\label{sec:binary:assess}

The pattern of data we observed is sufficiently precise and detailed
that extracting its full theoretical value requires more than
arbitrary statistical tests of simple null hypotheses --- e.g., the null
hypothesis that in the \target{exactly one}{some} condition, ratings
are the same for the worlds admitted by local enrichment as for those
excluded under both global and locally-enriched interpretations.  This
and other such null hypotheses can be rejected with high confidence.
Instead, to characterize the patterns of inference that give rise to
the observed data, we use a model-comparison approach. In particular,
we evaluate four related models that each embody different
characterizations of linguistic meaning. By comparing these models, we
can gain insights into the aspects of each that contribute to
particular patterns of predictions.

For all the models, we take as given the literal semantics described
in \tabref{tab:grammar}, as well as the following features of the
context:
%
\begin{examples}
\item\label{expmod}
  \begin{examples}
  \item $\Domain = \set{\playera, \playerb, \playerc}$
  \item $\Worlds = $ the set in \eg{conds}

    \marginnote{Added the missing null message to $\Messages$ -- thanks R2!}

  \item\label{expformulae} $\Messages =
    \setlength{\arraycolsep}{2pt}
    \set{
      Q(\word{player})(\word{hit}(S\negthinspace(\word{shot}))) :
      \begin{array}{l}        
        Q \in \set{\word{exactly one}, \word{every}, \word{no}}, \\
        S \in \set{\word{every}, \word{no}, \word{some}}
      \end{array}} \cup \set{\nullmsg}$
  \item $\Costs(\nullmsg) = 5$; $\Costs(\msg) = 0$ for all $\msg \in \Messages{-}\set{\nullmsg}$  
  \item Flat state prior: $\StatePrior(w) = \StatePrior(w')$ for all $w, w' \in \Worlds$
  \item Flat lexicon prior: $\LexPrior(\Lex) = \LexPrior(\Lex')$ for all $\Lex, \Lex' \in \LexSet$
  \end{examples}
\end{examples}

\marginnote{Note about the null message cost, with a link to the new appendix.}

The domain $\Domain$ and worlds $\Worlds$ come directly from our human
experiment. Similarly, the set of messages $\Messages$ corresponds to
\eg{expmsgs}, with some adjustments to keep the logical grammar
simple. We stipulate flat priors and even costs (other than the null
message). As noted in \secref{sec:model}, we do not have empirical
estimates for these values; though better fits to the human data can
be achieved by adding assumptions about them, this risks overfitting
to the particular data we have and thus overstating the true accuracy
of the models. The value $\Costs(\nullmsg) = 5$ was chosen
arbitrarily; \appendixref{app:paramexplore} explores a wide range of
values for it.

The models we consider are defined as follows:
%
\begin{examples}
\item 
  \begin{examples}
  \item \tech{Literal semantics}: the predicted values are the output
    of $\listenerZero$, as in \subeg{agents}{l0}, run on the messages
    defined in \subeg{expmod}{expformulae}.
  \item \tech{Fixed-lexicon pragmatics}: the predicted values are the
    output of the uncertainty listener \subeg{agents}{L}, but all the
    lexical items have only themselves as refinements, so that the
    reasoning is entirely in terms of the base lexicon in
    \tabref{tab:grammar}.
  \item \tech{Unconstrained refinement}: the inferences of the
    uncertainty listener \subeg{agents}{L} with $\Refine(\word{some})
    = \wp(\sem{\word{some}}){-}\emptyset$
  \item \tech{Neo-Gricean refinement}: as in `Unconstrained
    refinement', but with $\Refine(\word{some}) =
    \set{\sem{\word{some}}, \sem{\word{some and not all}}}$.
  \end{examples}
\end{examples}

These models represent a broad range of approaches to linguistic
meaning. The first neglects pragmatics entirely (the model includes a
contextual prior over states, but we define it as flat). The second is
a version of the rational speech acts (RSA) model of
\citet{Frank:Goodman:2012} and \citet{Goodman:Stuhlmuller:2013}. RSA
has been shown to capture a broad range of scalar implicatures, but it
is known to be limited in its ability to derive manner implicatures
and certain classes of embedded implicature
\citep{Bergen:Goodman:Levy:2012,Bergen:Levy:Goodman:2014}. The final
two models are full versions of the one we presented in
\secref{sec:model}. They represent opposite ends of the spectrum of
non-trivial refinements. We saw in connection with
\tabref{tab:subjects} and \tabref{tab:subjects-ALTstyle} that there
might be empirical value in greatly constraining the space of
refinements.
\marginnote{Slight elaboration to address reviewer questions about the assessments.}

We employ three methods of comparison: Pearson's correlation
coefficient, which measures the linear correlation between the human
responses and the model predictions; Spearman's rank correlation
coefficient, which assesses how closely the human responses and model
responses are aligned in terms of the rankings they predict; and the
mean-squared error (MSE) of the model predictions as compared with the
human responses, which summarizes the distance of the predictions from
the human behavior. The use of these three measures allows us to
detect models that reproduce quantitative correspondence (Pearson
correlation), qualitative correspondence (Spearman correlation), and
absolute fit between models and data. We find that the Spearman
measure is often the most illuminating, since our fundamental goal is
to reproduce the preference orderings revealed by the human responses.
However, the three measures together yield a succinct multidimensional
summary of how the models fare, and the same measures can be applied
to particular target sentences to achieve more fine-grained insights.

\todo{Everyone except Chris had great ideas about how to explicate and justify our link between model probabilities and human responses. Those could go here!}

Our model predictions are conditional probability distributions over
states given messages, and hence constrained to be in the range
$[0,1]$ and to sum to $1$. In contrast, our human responses are binary
true/false judgments. To align these values, we rescale the human
responses: if $x^{s}$ is the $10$-dimensional vector of
percentage-true human responses for target sentence $s$, then each
$p^{s}$ is the vector of normalized values for that sentence, defined
so that $p^{s}_{i} = x^{s}_{i}/\sum_{j=1}^{10}(x^{s}_{j})$. This
simply normalizes the responses into a conditional probability
distribution over states given messages. The one noteworthy thing
about this calculation is that, because it is done on a per-sentence
basis, it is not a simple linear rescaling, and so it affects all of
our assessment metrics when applied to multiple sentences at
once. However, we regard it as the minimal viable linking hypothesis
relating our model with our experimental data.

\begin{figure}[!t]
  \centering
  \includegraphics[height=0.92\textheight]{fig/allmodels-binary}
  \caption{Analysis by target sentence, comparing model predictions
    with human responses.}
  \label{fig:exp-analysis}
\end{figure}

\marginnote{New end-of-para note about how to get `global' implicatures, with R3 in mind.}

\Figref{fig:exp-analysis} summarizes the models' predictions alongside
the human responses. The predicted values are largely aligned for the
examples without \word{some} in the object position. Where \word{some}
occurs embedded, the models diverge in qualitative terms. For
\target{every}{some}, the patterns are broadly similar, but only
`Neo-Gricean uncertainty' is able to mirror the preference ordering of
responses seen in the human data. For \target{exactly one}{some}, only
the two uncertainty models are able to predict local enrichment, in
that only they assign high probability to the crucial worlds that are
false on the literal construal: \world{NSA} and \world{SAA}. The
`Literal semantics' and `Fixed-lexicon pragmatics' models are unable
to predict the salience of these construals. Similarly, only the two
uncertainty models predict \target{none}{some} to have embedded
enrichments leading to acceptability for \world{NNA}, \world{NAA}, and
\world{AAA}. In broad strokes, we can say that `Fixed-lexicon
pragmatics' predicts only `global' implicatures, those that \CFS\
would obtain with unembedded exhaustification, whereas the two
uncertainty models simulate embedded exhaustification (though without
predicting it to be the most preferred option, in line with our human
responses).

\begin{table}[!t]
  \centering
  \begin{tabular}[c]{r c c c}
    \toprule
    & Pearson & Spearman & MSE \\
    \midrule
    Literal semantics         & .94 & .76 & .0065 \\
    Fixed-lexicon pragmatics  & .92 & .76 & .0079 \\
    Unconstrained uncertainty & .95 & .79 & .0038 \\
    Neo-Gricean uncertainty   & \graycell{.96} & \graycell{.81} & \graycell{.0034}\\
    \bottomrule   
  \end{tabular}
  \caption{Overall assessment.}
  \label{tab:binary:overall}
\end{table}

\Tabref{tab:binary:overall} summarizes our overall quantitative
assessment. All of the correlations are extremely high, and the MSE
values are extremely low. This is reassuring about the general utility
of all of these models for predicting human judgments. Comparing the
values is arguably more illuminating. A few patterns stand out.
First, `Fixed-lexicon pragmatics' performs the least well overall.
Since it has been shown to add substantial value in other areas of
language and cognition, we conclude that its particular approach to
enrichment is at odds with the patterns for embedded implicatures.
The precise causes are hard to pin-point, but the fact that our target
implicatures are not always enrichments of the literal content is
surely part of the problem.  Second, while `Unconstrained uncertainty'
is better than literal semantics by the Spearman and MSE metrics, it
falls behind it on the Pearson metric. We saw in connection with
\tabref{tab:subjects} that the unintuitive lexica that this model
entertains can lead to implausible final patterns. We conjecture that
this is the root cause of the model's poor performance here as well.
Third, neo-Gricean uncertainty achieves the best results across all
three of our measures. Here again, this is consistent with our
expectations based on the large illustrative example from
\secref{sec:illustrations}, where we saw that this constrained,
lexically-driven approach to choosing refinements resulted in the best
quantitative and qualitative pattern.

The overall analysis given in \tabref{tab:binary:overall} understates the
value of both uncertainty models when it comes to the distribution of
embedded implicatures. Our target sentences provide relatively little
space for pragmatic enrichment; in \figref{fig:binary:results}, the left
and middle columns essentially have only literal interpretations,
leaving just the right column for our pragmatic models to
shine. What's more, our qualitative review of
\figref{fig:exp-analysis} suggests that the right column examples
reveal major distinctions. It's thus worth assessing them
quantitatively in isolation. The results of such an assessment are in
\tabref{tab:crucial-items}.  The most dramatic pattern is that the two
fixed-lexicon models are unable to capture the patterns for embedded
implicatures in the non-monotone and downward monotone
environments. In contrast, both uncertainty models capture the
patterns. These tight fits are evident in \figref{fig:exp-analysis},
and it is reassuring to see them reflected in our assessment measures.

It is also striking that the literal model is competitive for
\target{every}{some}. This probably traces to the experimentally
inconvenient fact that the enriched meaning entails the literal one,
making a literal semantics a generally safe bet. It should be noted
also that the Spearman coefficient best reflects the ability of a
model to capture the preference ordering in the human data, so the
`Neo-Gricean uncertainty' model might still be the favored choice for
this case.

Finally, it seems that neither uncertainty model is clearly superior
to the other for these data.  This suggests to us that we have not yet
found precisely the right approach to refinements (alternatives). It
is tempting to try additional refinement sets to find a single model
that wins decisively for all the target examples. We are wary of doing
this because, as noted above, it runs the risk of overfitting to our
experimental responses; we could easily engineer our own success.
However, this is nonetheless a fruitful avenue for future exploration
if paired with additional experiments for further
validation. \Appendixref{app:paramexplore} offers additional findings
that are relevant for such investigations.

\begin{table}[t]
  \centering
  \setlength{\tabcolsep}{4pt}
  \newcommand{\rcoldiv}{\hspace{44pt}}
  
  \begin{tabular}[c]{r rrr @{\rcoldiv} rrr  @{\rcoldiv} rrr }
    \toprule
    & 
    \multicolumn{3}{c}{\target{every}{some}}{\rcoldiv} & 
    \multicolumn{3}{c}{\target{exactly one}{some}}{\rcoldiv} &
    \multicolumn{3}{c}{\target{no}{some}} \\
    & 
    P & S & MSE & 
    P & S & MSE & 
    P & S & MSE \\
    \midrule  
    Literal       & \graycell{.99} &            .86 & \graycell{.0002} &            .80 &            .70 &            .0180 &            .88 &            .52 &            .0346 \\
    Fixed-lexicon &            .93 &            .85 &            .0027 &            .80 &            .70 &            .0179 &            .88 &            .52 &            .0346 \\
    Unconstrained &            .88 &            .84 &            .0043 & \graycell{.98} & \graycell{.94} & \graycell{.0007} &            .76 &            .57 &            .0097 \\
    Neo-Gricean   &            .82 & \graycell{.88} &            .0087 &            .94 &            .87 &            .0036 & \graycell{.93} & \graycell{.89} & \graycell{.0028} \\
    \bottomrule
  \end{tabular}
  \caption{Assessment of crucial items. `P' = `Pearson'; `S' = `Spearman'.}
  \label{tab:crucial-items}
\end{table}

Our model's performance is sensitive to the space of competitor
messages, so it is worth asking how robust these findings are to
changes in this area. We have found that the basic pattern is robust
to a number of changes to the space of quantifiers.  The only
noteworthy finding we have to report in this regard is that allowing
\word{exactly one} into object position has a major impact: while
\world{SSS} remains the best-guess inference for the message
\target{every}{some} in this setting, \target{exactly one}{some} and
\target{no}{some} effectively lose their embedded implicature
readings.  This makes intuitive sense given the nature of the model:
if the speaker has the option to choose \word{exactly one of his
  shots}, and that form is equally costly, then surely her avoidance
of that form in favor of \word{some of his shots} is a signal that she
regards the local enrichment as infelicitous. As \word{exactly one} is
made more costly, it becomes a less salient option, and embedded
implicatures begin to reemerge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}\label{sec:conclusion}

With this paper, we sought a synthesis between Gricean accounts of
pragmatic reasoning and grammar-driven ones like that of
\citet{ChierchiaFoxSpector08}. It seems to us inevitable that both
grammar and interaction will play leading roles in the final theory of
these phenomena; at some level, all participants in the debate
acknowledge this. Our achievement is to unify the crucial components
of these approaches in a single formal model that makes quantitative
predictions.

The key components of the model we develop are compositional lexical
uncertainty and recursive modeling of speaker and listener agents
\citep{Bergen:Levy:Goodman:2014}. The lexical uncertainty property is
in evidence in \citeauthor{ChierchiaFoxSpector08}'s account as well,
in the form of underspecified logical forms with context dependent
meanings. Our model has similar formal mechanisms but also offers an
account of how discourse participants reason under this persistent
linguistic uncertainty. This leads to an important conceptual point:
not all underspecification has to be resolved in order for robust
pragmatic enrichment to take place.

The recursive reasoning of our model is characteristic of both Gricean
approaches and signaling systems approaches; our model shares formal
properties of both but makes quantitative predictions of the sort that
can be correlated with human preferences in communication. There are
by now many models in the same family as ours (see, e.g.,
\citealt{CamererHo:2004,Jaeger:2011,Smith:Goodman:Frank:2013,Kao-etal:2014,Jaeger:Franke:2014}),
so further exploration is likely to yield an even more nuanced
picture.

In addition, we saw that the space of refinements (alternatives) has a
significant impact on the final predictions. It would thus be
worthwhile to further explore different notions of refinement, seeking
better fits with our own experimental patterns and then validating
those conclusions in follow-up experiments using our experimental
items, or applying the resulting models in new domains.  We have made
publicly available all the data and code associated with this paper in
an effort to encourage these new strands of theory development and
quantitative assessment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{appendix}

\section{Parameter exploration}\label{app:paramexplore}

As we discussed in \secref{sec:agents}, the definition of our model
naturally suggests at least two extensions: (i) a temperature
parameter $\lambda$ modulating the speaker's inferences, and (ii)
further iteration beyond the level of $\UncertaintyListener$. The full
extended form of the model is defined as follows, again drawing on the
objects and notational conventions established in \secref{sec:agents}:
%
\begin{examples}
\item\label{agents-extended}
  \begin{examples}
  \item
    $\listenerZero(\state \given \msg, \Lex) \propto 
    \StatePrior(\state) \text{ if } \state \in \Lex(\msg), \text{ else } 0$

  \item 
    $\speakerOne(\msg \given \state, \Lex) \propto
    \exp
    \left(\lambda \left(
      \log\listenerZero(\state \given \msg, \Lex)
      - 
      \Costs(\msg)\right)
    \right)$
    
  \item
    $\UncertaintyListener[1](\state \given \msg) 
    \propto 
    \StatePrior(\state)
    \sum_{\Lex \in \LexSet}
    \LexPrior(\Lex)
    \speakerOne(\msg \given \state, \Lex)$

  \item $\UncertaintySpeaker{k}(\msg \given \state) \propto
    \exp
    \left(\lambda \left(
      \log\UncertaintyListener[k-1](\state \given \msg)
      - 
      \Costs(\msg)
    \right)\right)$
  \hfill (for $k > 1$)

  \item $\UncertaintyListener[k](\state \given \msg) \propto 
    \UncertaintySpeaker{k}(\msg \given \state) \StatePrior(\state)$
    \hfill (for $k > 1$)
  \end{examples}
\end{examples}

\begin{table}[t]
  \centering
  \setlength{\tabcolsep}{12pt}
  \begin{tabular}[c]{ *{6}{r} }
    \toprule
    &&& $\Costs(\nullmsg)$ & $\lambda$ & $k$\\
    \midrule
    \multirow{3}{*}{Literal semantics} & Pearson & .94 &  &  & \\
    & Spearman & .76 &   &  & \\
    & MSE & .0065 &  &  & \\
    \addlinespace[1ex]
    \multirow{3}{*}{Fixed lexicon pragmatics} & Pearson & .93 & 1 & .1 & 1\\
    & Spearman & .76 & 0 & .2 & 1\\
    & MSE & .0069 & 1 & .1 & 1\\    
    \addlinespace[1ex]
    \multirow{3}{*}{Unconstrained uncertainty} & Pearson & .97 & 1 & .1 & 1\\
    & Spearman & .80 & 1 & .1 & 1\\
    & MSE & .0022 & 1 & .1 & 1\\
    \addlinespace[1ex]
    \multirow{3}{*}{Neo-Gricean uncertainty} & Pearson & \graycell{.98} & 1 & .1 & 1\\
    & Spearman & \graycell{.81} & 1 & .2 & 1\\
    & MSE & \graycell{.0018} & 1 & .1 & 1\\
    \bottomrule
  \end{tabular}
  \caption{Best models found in hyper-parameter exploration, as 
    assessed against the binary-response experiment. 
    The literal listener is not affected
    by any of the parameters explored.}\label{tab:grid}
\end{table}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\textwidth]{fig/allmodels-paramexplore-binary}
  \caption{The crucial target sentences comparing the human data with
    $\UncertaintyListener[1]$, using parameters in the range that seem
    to be nearly optimal for all of these models: $\lambda=0.1$ and
    $\Costs(\nullmsg)=1$.}
  \label{fig:optimal}
\end{figure}

From the perspective of this model, our decision to set $\lambda=1$
and focus on $\UncertaintyListener[1]$ might appear arbitrary.  In
addition, even from the perspective of our simpler model, our decision
to fix the cost of the null message at $5$ for all simulations and
assessments was arbitrary. It is therefore worth exploring other
settings for these hyper-parameters. To do this, we conducted a
comprehensive grid search of the following values:
%
\begin{examples}
\item
  \begin{examples}
  \item $\lambda$: $[0.1, 2]$ in increments of $.1$, and $[3,5]$ in increments of $1$
  \item $\UncertaintyListener[k]$ for $k \in \set{1,2,3,4,5,6}$
  \item $\Costs(\nullmsg) \in \set{0, 1, 2, 3, 4, 5, 6}$
  \end{examples}
\end{examples}
%
The grid search explores the full cross product of these values for
each of our four models. For each setting, we conduct our standard
model assessment against the data from our main (binary-response)
experiment. \Tabref{tab:grid} reports the best values for each of our
four models, along with the minimal parameter settings that deliver
those values. These results are consistent with our fundamental
assessment of these models (\secref{sec:binary:assess}). They also
suggest that further iteration beyond $\UncertaintyListener[1]$ is not
necessary \citep{Vogel-etal:2014}, and that $\lambda=1$ is too
aggressive. (The cost of the null message has a relatively small
impact on the outcomes.) \Figref{fig:optimal} offers a finer-grained
look at how these preferred settings affect outcomes for the crucial
target items involving embedded \word{some}. The literal column is
identical to the one in \figref{fig:exp-analysis}. The others are
subtly different in ways that achieve a better match with the human
data. For instance, the optimal parameters assign more probability to
\world{AAA} in the \target{no}{some} condition, which better matches
the human responses. Overall, though, the contrasts between items are
dampened relative to the version of the model with $\lambda=1$.

\marginnote{Mike, let me know if this isn't responsive to your suggestions.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Likert-scale experiment}\label{app:likert}

We also conducted a version of the binary-response experiment
discussed in \secref{sec:binary} using a Likert-scale for the response
categories.  Our rationale for using this scale is that it allows
enough space for participants to both register a truth-value
assessment and convey information about the quality of the
report. This appendix reports briefly on this experiment. It yielded
results identical in all important respects to those from our main
experiment.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{fig/basketball-pilot-2-11-14-results-parsed}
  \caption{Likert-scale experimental results. Mean ratings by sentence with bootstrapped 95\% confidence intervals.}
  \label{fig:likert:results}
\end{figure}

\subsection{Methods}

\subsubsection{Participants}

The experiment had 300 participants, all recruited with Amazon's
Mechanical Turk. No participants or responses were excluded.

\subsubsection{Materials}

The displays were identical to those in \figref{fig:binary:materials},
generated by the same procedures, but with the binary response
categories replaced with a seven-point Likert scale ranging from `Bad
description' to `Good description'. The target sentences were the ones
in \eg{expmsgs}, and the conditions were as in \eg{conds}. The same 23
fillers were used.

\subsubsection{Procedure} 

After reading our consent form, participants were given the cover
story in \eg{coverstory}, and then they completed the same three
training items as were used in our main experiment. The design was
again between-subjects.  Each sentence received a total of 300
responses. For the target sentences, each sentence--world pair
received between 19 and 44 responses (mean 30); this variation derives
from our randomized procedure for assigning worlds to sentences.

%=====================================================================

\subsection{Results}

\Figref{fig:likert:results} summarizes the responses by target sentence
and world of evaluation. The results mirror those
seen in \figref{fig:binary:results} in all important respects.
%
For our key theoretical comparisons, we again report significance
levels using the nonparametric Mann--Whitney U test. In the
\target{every}{some} case, the highest ratings came in the \world{SSS}
world. Worlds \world{SSA} and \world{SAA} received the next highest
ratings (lower than \world{SSS}; both at $p<0.001$). Of all the
literally true worlds, \world{AAA} received the lowest rating (lower
than \world{SSA} and \world{SAA}; both at $p<0.05$).
%
For the \target{exactly one}{some} item, the highest ratings are again
in the \world{NNS} condition, where it is true under its literal and
locally enriched construals, but it also received high ratings in the
two worlds where it is true only with local enrichment: \world{NSA}
and \world{SAA}, which were both higher at $p<0.05$ than in
\world{SSA}, the world yielding the highest rating among those in
which the sentence is false both literally and under all possible
enrichments. As before, the strictly truth-conditional interpretation
seems to be salient as well.
%
Finally, we also find evidence for local enrichment under
\target{no}{some}. Condition \world{NNN} received the highest average
ratings, suggesting a preference for a literal construal, but the
ratings are high for the conditions requiring local enrichment:
\world{NNA}, \world{NAA}, and \world{AAA}. The confidence intervals
are wide, but a pooled comparison of \set{\world{NNS},\world{NSA}}
with \set{\world{NNA},\world{NAA},\world{AAA}} shows the latter set to
be significantly higher-rated; $p = 0.006$.

%=====================================================================

\subsection{Model assessment}

\Tabref{tab:likert:overall} summarizes our model assessment. This
assessment was done with identical settings and procedures to those
reported in \secref{sec:binary:assess}, with one exception: since the
minimal Likert value is $1$, we subtract $1$ from all scores when
transforming them into the by-message normalized probability space of
the model. Neo-Gricean uncertainty again emerges as the best model.

\begin{table}[t]
  \centering
  \begin{tabular}[c]{r c c c}
    \toprule
    & Pearson & Spearman & MSE \\
    \midrule
    Literal semantics         & .94 & .76 & .0079\\
    Fixed-lexicon pragmatics  & .92 & .75 & .0094\\
    Unconstrained uncertainty & .93 & .79 & .0052\\
    Neo-Gricean uncertainty   & \graycell{.95} & \graycell{.80} & \graycell{.0046}\\
    \bottomrule   
  \end{tabular}
  \caption{Overall assessment of the Likert-scale experiment.}
  \label{tab:likert:overall}
\end{table}

\end{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{sp}
\setlength{\bibsep}{0pt}
\bibliography{embedded-scalars-bib}

\end{document}

