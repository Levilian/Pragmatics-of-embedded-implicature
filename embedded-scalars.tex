\documentclass{article}

\input{preamble}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Embedded implicatures, compositional uncertainty, and pragmatic reasoning}
\author{The pragmateurs}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview}\label{sec:introduction}

\citet{Grice75} defined conversational implicatures as social,
cognitively complex meanings that discourse participants create
jointly in interaction. Recent grammar-driven accounts are framed in
opposition to this conception, especially for scalar implicatures
(SIs).  For example, \citet{ChierchiaFoxSpector08}{2316} write, ``the
facts suggest that SIs are not pragmatic in nature but arise, instead,
as a consequence of semantic or syntactic mechanisms''. The ensuing
debates have stimulated new theoretical and empirical insights,
pushing researchers to identify and evaluate previously unnoticed
consequences of the two broad positions.

Our position is that grammar-driven accounts and Gricean accounts are
not truly in opposition, but rather offer complementary insights.
When communicating in natural language, people are relying on
grammatical conventions to try to identify others' intentions and
convey their own intentions. All sides in the debate acknowledge this.
\posscitet{Grice75} maxim of manner embraces a role for language;
neo-Gricean positions expand this role into areas Grice addressed with
the maxims of quantity, quality, and relevance; \citet{Sperber95} and
\citet{Bach94} characterize many kinds of pragmatic enrichment as
inferences about logical form structure; and
\citet{ChierchiaFoxSpector08} rely on broadly Gricean pressures to
explain how speakers and listeners coordinate on whether to assume
``implicature-rich'' logical forms or more literal ones.

Much of the debate between Gricean and grammar-driven accounts has
centered around apparently semantically embedded implicatures ---
cases where a pragmatically enriched interpretation seems to become
part of the compositional semantics. Many instances of apparently
embedded implicatures have been shown to have straightforward Gricean
accounts \citep{Russell06,Geurts09}. \citet{Chemla:Spector:2011} work
through a wide range of such cases involving scalar terms embeded in
the scope of quantified phrases. In their experiments, such embeded
terms reliably give rise to implicature rich interpretations. They
show that many such patterns are amenable to Gricean treatments.
However, occurrences of scalar terms in the scope of non-monotone
quantifiers remain recalcitrant on this view because they seem to
support implicature-rich interpretations, but they do not involve the
entailment relations needed for standard Gricean pragmatic enrichment
to deliver the right results. This appears to be a formidable
challenge for classical Gricean approaches.

In this paper, we reproduce the central results of
\citet{Chemla:Spector:2011} using simpler and more naturalistic
experimental stimuli and a more direct method of interpreting
subjects' responses. We hope that these results bolster
\citeauthor{Chemla:Spector:2011}'s original findings and help to
address the skeptical reactions of \citet{geurts-vantiel:2013:scalar}.
In our view, this evidence points to a role for compositional
semantics in understanding implicatures. However, we do not stop
there.  We propose a model that embraces the compositional insights
\citeauthor{ChierchiaFoxSpector08} and also makes predictions about
which sentences speakers will choose and what inferences listeners
will make based on those messages. In other words, rather than leaving
the task of disambiguation (logical form selection) outside of the
model, we bring it in and make predictions about it, seeking to retain
the best aspects of Gricean accounts while paying close attention to
the subtle details of semantic composition.  

Our model is a simple extension of the lexical uncertainty model of
\citet{Bergen:Goodman:Levy:2012} and \citet{Bergen:Levy:Goodman:2014},
which defines both production and interpretation as a recursive
process in which speakers and listeners reason jointly about the state
of the world and the nature of the linguistic system they are
using. \citeauthor{Bergen:Levy:Goodman:2014} show that this model
captures a wide range of well-studied classes of implicature.  Our
extension simply allows for greater diversity in the semantic lexicon
and includes more complex aspects of semantic composition.  In this
model, implicatures are semantic \emph{and} pragmatic, a departure
from Grice's particular conception of pragmatic meaning but
well-aligned with his broader theory of meaning and intention.

In view of our experimental results, the chief advantage of this model
is that it makes quantitative predictions that are easily and
rigorously linked with our human response patterns.  In other words,
the model makes predictions not only about what is possible in terms
of pragmatic enrichment but also about how likely those inferences
are. Thus, we are able to show not only that the model captures the
qualititive pattern of implicature behaviors that
\citeauthor{Chemla:Spector:2011} characterized, but also that its
predictions are highly correlated with actual inferential behavior in
context.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conversational implicature and pragmatic enrichment }\label{sec:implicature}

\posscitet{Grice75} original definition of conversational implicature
describes what is essentially an act of social cognition. The original
definition is somewhat underspecified, and fleshing it out into a
precise formulation is challenging \citep{Hirschberg85}, but the
guiding idea seems clear.  The listener assumes that that the speaker
is cooperative, in the Gricean sense of rational interaction. However,
the listeer is confronted with an utterance $U$ with content $p$ that
falls short of this assumption. However, if the listener posits that
the speaker actually intended different (often stronger, but
potentially different or even conflicting) proposition $q$, then $U$
is reconciled with the assumption of cooperativity. What researchers
call the implicature is $q - p$, the content attributable to pragmatic
inference rather than to the literal semantics.

The model that we develop does not depend on an independently
formulated definition of implicature, but rather seeks to derive such
meanings from more basic considerations about how speakers and
listeners reason about each other whenever they interact. The \CFS\
model is similar in this regard: `implicature' could be seen as an
informal label for a certain class of logical forms on that view,
rather than a conceptual primitive. Thus, we will not try to make the
above definition more rigorous. Rather, we just use it to further
articulate the central empirical focus of this paper --- embedded
scalar terms --- and the challlenges they pose for Gricean accounts.

Scalar implicatures arise when the imperative `Be as informative as is
required' (a subclause of the maxim of quantity) is in tension with
another pragmatic pressure. The opposing force can take many forms,
for example, relating to considerations of politeness, discretion,
secrecy, but it is usually attribited to the maxim of quality, which
enjoins speakers to say only what they have strong positive evidence
for. For example, imagine a sportscaster who has observed the outcome
of a tournament round and is reporting on the outcome to a group that
didn't see the outcome. If the sportscaster says \eg{some}, then she
will likely implicate that Player A did not make all of his shots.
%
\begin{examples}
\item\label{some} Player A made some of his shots.
\end{examples}
%
This follows from a straightforward application of the above ideas. We
assume the sportscaster is knowledgeable and forthcoming about the
events. Why, then, did she opt for a weak statement like \word{Player
  A made some of his shots} when a stronger statement \word{Player A
  made all of his shots} would have been more informative. Given our
other assumptions, it must be that the speaker was prevented from
using this stronger form because she does not know it to be
true. Together with our assumption that she observed the full outcome,
she can only lack knowledge of this fact because it is false. In this
way, the speaker's message is enriched and her cooperativity remains
unimpeached. There are many proposals in the literature to formalize
this directly, often by reasoning about context-specific clsses of
alternative utterances $U'$ to the original utterance $U'$, relying on
their comparative informativity, markedness, and salience to compute
the target implicature from broadly Gricean premises
\citep{Horn72,Gazdar79b,Gazdar79a,SchulzVanRooij06}. The common theme
running through all of these accounts is that the implicature is
accessible because it is a refinement that strictly entails the
original literal content.

The above informal reasoning extends to examples like \eg{everysome},
in which \word{some} is in the scope of a universal quantifier.
%
\begin{examples}
\item\label{everysome} Every player made some of his shots.
\end{examples}
%
With the same contextual assumptions in place as before, this can be
enriched in context to convey that every player made some but not all
of his shots. Call this proposition $q$. It unilaterally entails the
proposition $p$ expressed by \eg{everysome}, so a listener might
wonder why it was not expressed and thereby be led to the conclusion
that it is false. Making this reasoning explicit in terms of Gricean
comparisons with alternative utterances is challenging, but the same
fundamental logic holds in both \eg{some} and \eg{everysome}:
implicature reasoning allows us to refine the general literal content
$p$ into a meaning $q$ that strictly entails $p$.

\citet{Chemla:Spector:2011} home in on this common theme in scalar
implicature calculation and use it to probe the limits of the Gricean
framework. Examples like \eg{exactlyonesome} drive their account.
This is a minimal variant of \eg{everysome} with subject universal
determiner \word{every} replaced by \word{exactly one}.
%
\begin{examples}
\item\label{exactlyonesome} Exactly one player made some of his shots.
\end{examples}
%
Many people have the intuition that \eg{exactlyonesome} can be used to
describe a situation in which there is exactly one player who scored
some but not all of his shots, which is consistent with some players
having scored all of their shots. The reading is easy to characterize
intuitively: one imagines that \word{some of his shots} has been
locally enriched to \word{some but not all of his shots}, and that
this enriched meaning is the semantic argument to the subject
quantifier. What makes this reading notably different from, e.g.,
\eg{everysome} is that it does not entail the literal reading. This
point is sufficiently important for the paper that it is worth
spelling it out in some detail. Suppose that we have two players, A
and B, and that we care (for present purposes) only about whether each
of them made none, some but not all, or all of his shots. We can
identify these worlds with labels like \texttt{NA}, which means that
player A made none of his shots and player B made all of his shots,
and \texttt{SS}, which means that both players made some but not all
of their shots. There are $2^{3} = 8$ such worlds. The literal
sematics of \eg{exactlyonesome} in this logical space would be the
proposition in \subeg{exactlyonesome-sem}{lit}, whereas the
proposition expressed by the enrichment \word{\ldots some but not all
  of his shots} would be \subeg{exactlyonesome-sem}{local}.
%
\begin{examples}
\item\label{exactlyonesome-sem}
  \begin{examples}
  \item\label{lit}   $\set{\texttt{NS}, \texttt{SN}, \texttt{NA}, \texttt{AN}}$
  \item\label{local} $\set{\texttt{NS}, \texttt{SN}, \texttt{NA}, \texttt{AN}, \texttt{SA}, \texttt{AS}}$
  \end{examples}  
\end{examples}
%
The local reading is a strict weakening of the literal reading, not an
enrichment of the sort that we saw before. This means that any theory
based on broadly Gricean notions of enrichment will fail to arrive at
\subeg{exactlyonesome-sem}{local}. Such theories tend to head
inexorably towards a refinement of \subeg{exactlyonesome-sem}{lit}
that excludes \texttt{NA} and \texttt{AN}, but they are incapable of
attenuating the meaning to include \texttt{SA} and \texttt{AS} (the
`global' reading in \citeauthor{Chemla:Spector:2011}'s terms).
Modifying an earlier design by \citet{Geurts:Pouscoulous:2009},
\citeauthor{Chemla:Spector:2011} use displays involving geometric
designs to see whether interpreters can access local-enrichment
readings of sentences like these. Their findings suggest that such
readings are robustly available. 

Skeptics of local enrichment have found grounds for challenging these
findings based on the methods used. For example, participants clearly
struggled with the complex visual displays, often giving low ratings
to true readings and sometimes giving high ratings to false ones. In
addition, \citeauthor{Chemla:Spector:2011} rely on additional
theoretical assumptions in order to link their theory to the response
patterns --- roughly, they must invoke an auxiliary hypothesis that
the more true construals are available for an ambiguous or
underspecified sentence, the more confident people will be in their
judgment that a sentence is true. Whether or not this assumption is
reasonable (we're inclined to think it is a reasoable response in the
face of uncertainty), the need to invoke it opens up further room for
criticism. To address these concerns, we report on a new version of
the experiment in \secref{sec:experiments} that uses simpler designs
and a more straightforward hypothesis linking responses to theoretical
interpretations. This experiment reproduces the core findings of
\citeauthor{Chemla:Spector:2011} studies, suggesting that these
sentence do in fact pose a serious problem for Gricean reasoning as
usual.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\CFS's grammar-driven model}\label{sec:grammar}

This section briefly reviews the grammar-driven model of \CFS\
\citep{ChierchiaFoxSpector08}.  The approach is directly inspired by
those of \citet{Chierchia01}, \citet{Sauerland01},
\citet{Spector:2007}, and \citet{Fox:2007,Fox:2009}. There are two
central pieces to the account: a generally available function $\ALT$
that maps denotations to their alternatives, and a covert
exhaustification operator $O$.

For $\ALT$, the relevant notion of alternative is familiar from
theories of questions and focus \citep{Groenendijk84,Rooth85,Rooth92}:
we can assume, as a default, that the alternatives for a meaning $t$
are some subset of the items in the same type-theoretic denotation
domain as $t$.  One can also imagine variants of this proposal in
which $\ALT$ operates over lexical items, rather than denotations, but
the denotational view will suffice here.  The function $\ALT$ is part
of context-dependent semantics: the discourse participants need to
coordinate on it just as they need to coordinate on the meanings of
deictic or discourse-bound pronouns, ellipsis sites, evaluation
standards, and the like.

The basic exhaustification operator is given in \eg{def:O}
\citep{Spector:2007,Fox:2007,Fox:2009,Magri:2009,ChierchiaFoxSpector08}.\footnote{This
  is not the operator that those authors ultimately favor, since it
  requires some implicit restrictions on allowable $\ALT$ functions in
  order to get the right inferences.  The final version has the same
  form as \eg{def:O} but further restricts $\ALT$ to alternatives that
  are \tech{innocently excludable}.}
%
\begin{examples}
\item\label{def:O}
  $O_{\ALT}(\varphi) = 
  \sem{\varphi} \sqcap 
  \forall q \in \ALT : (\sem{\varphi} \not\entails q) \entails \neg q$
\end{examples}
%
The $O$ operator maps an expression $\varphi$ to one that entails $p$
and excludes all of the meanings that are strictly stronger than
$\sem{\varphi}$. When dealing with truth-functional expressions, we
can regard $\sqcap$ as boolean conjunction and $\entails$ as
entailment, but the definition should be thought of as broad enough to
include any kind of partial ordering, which \seccitet{Hirschberg85}{4}
shows to be needed to capture the full range of `scalar' implicatures.

Part of the case for a grammar-driven view is that it uses pieces of
semantic theory that are independently useful. In particular,
exhaustification is at the heart of \posscitet{Groenendijk84} theory
of questions and their answers (see also
\citealt{JohnMcCarthy80}). The above operator is a common proposal for
the meaning of \word{only} (for discussion:
\citealt{Rooth96,Buring01,BeaverClark08}).  \citet{SchulzVanRooij06}
use exhaustification for implicature calculation (see also
\citealt{deJagerVanRooij07}).  (For critical discussion, see
\citealt{Alonso-Ovalle:2008} and \citealt{Gajewski:2012}.) While \CFS\
are cautious about making direction connections between $O$ and these
other phenomena (p.~2304), the correspondences are nonetheless
noteworthy.

Those are the technical pieces. The proposal can then be summarized
easily: $O$ operators can appear anywhere in the logical form of a
sentence, perhaps subject to additional restrictions and general
preferences (see \CFS: $\S$4.6). To see the effects that this could
have, let's return to the examples involving \word{some} that we
reviewed in \secref{sec:implicature}. Simplifying slightly, let's
suppose that \word{some shot} denotes the set of sets in \eg{someshot}
--- the set of all sets that have non-empty intersect with the set of
shots.
%
\begin{examples}
\item\label{someshot} $\sem{\word{some shot}} = \set{Y : \sem{\word{shot}} \cap Y \neq \emptyset}$
\end{examples}
%
Consider a domain of three entities $\set{a,b,c}$, and assume that
$\sem{\word{shot}} = \set{a,b}$. Then the above is equivalent to the
set of sets in \eg{someshotex}:
%
\begin{examples}
\item\label{someshotex}
  $\set{\set{a}, \set{b}, \set{a,b}, \set{a,c}, \set{b,c}, \set{a,b,c}}$
\end{examples}
%
Now suppose that $\ALT(\word{some shot})$ is defined as follows:
%
\begin{examples}
\item\label{altsome} $\ALT(\word{some shot}) =  
  \set{
    \sem{\word{some shot}}, 
    \sem{\word{every shot}}, 
    \sem{\word{no shot}}
  }$
  \begin{examples}
  \item $\sem{\word{some shot}}$ as in \eg{someshot}
  \item $\sem{\word{every shot}} = \set{Y : \sem{\word{shot}} \subseteq Y} = \set{\set{a,b}, \set{a,b,c}}$
  \item $\sem{\word{no shot}} = \set{Y : \sem{\word{shot}} \cap Y = \emptyset} = \set{\set{c}}$      
  \end{examples}
\end{examples}
%
The presence of $\sem{\word{some shot}}$ has no effect because it is
identical to the input. Similarly, all quantifiers that are weaker
than the input have no effect if included in the $\ALT$ set. The
presence of $\sem{\word{no shot}}$ has no effect because it
contradicts the input. The presence of $\sem{\word{every shot}}$ will,
though, be meaningful, as long as we assume that
$\sem{\word{shot}} \neq \emptyset$.  In that case,
$O_{\ALT}(\word{some shot})$ will denote the following subset of
\eg{someshotex} in our small domain:
%
\begin{examples}
\item\label{altsomeshotex}
  $\set{\set{a}, \set{b}, \set{a,c}, \set{b,c}}$
\end{examples}
%
This is equivalent to the intersection of $\sem{\word{some shot}}$ and
the complement of $\sem{\word{every shot}}$ in the powerset of the
domain.  In other words, it expresses \word{some, not all}, the
intuitively implicature-rich interpretation. And because $O_{\ALT}$ is
embeddable, syntactic constituents like $O_{\ALT}(\word{some shot})$
can appear in the scope of quantifiers.  Implicature-rich versions of
\eg{some}, \eg{everysome}, and \eg{exactlyonesome} are thus available
--- potentially usable by speakers and inferrable by listeners just
like any other semantic resolution for an underspecified form in
context.

As we noted in the introduction, \CFS\ draw a firm rhetorical
distinction between their proposal and the Gricean approach to
pragmatics. They state, ``the goal of this paper is to challenge the
neo-Gricean approach to SIs'' (p.~2303), and later they write that
``the facts suggest that SIs are not pragmatic in nature but arise,
instead, as a consequence of semantic or syntactic mechanisms''
(p.~2316). The sense in which their account reflects this position is
clear: to characterize implicatures, we need not consider the
interactional setting or try to model the speaker and hearer. Rather,
we can just describe a specific class of logical forms.

However, this position tempered by \CFS's pervasive appeals to Gricean
reasoning.  The authors' specific examples are generally placed in
contexts that support the target implicatures by ensuring that they
are relevant, informative, and truthful.  They concede that ``aspects
of the Gricean picture are sound and effective'' (p.~2299). And, in
summarizing their account, they make explicit the role that Gricean
pragmatics must play in helping discourse participants to coordinate
on the right logical forms:
%
\begin{quote}
  one can capture the correlation with various contextual
  considerations, under the standard assumption (discussed in the very
  beginning of this paper) that such considerations enter into the
  choice between competing representations (those that contain the
  operator and those that do not). (p.~2317)
\end{quote}

The fundamental coordination problem remains, then, though its precise
form is different. In the context of \CFS's thery, the discourse
participants must coordinate on the nature of the function $\ALT$.
Similarly, because the language permits silent, embedded $O$ operators
in many positions, the speaker's signal always underdetermines her
intended message; a given surface form $U$ might be consistent with
logical forms that convey implicatures and those that don't. The
speaker must therefore rely on the listener to select the right one.
From this perspective, implicature calculation amounts to reasoning
about which logical form was intended. How this coodination happens
has not been a focus of grammar-driven accounts, but the above
quotation suggests that communicative pressures like those
\citet{Grice75} identified guide the process.

Summarizing so far, we have evidence from
\posscitet{Chemla:Spector:2011} experiments that some implicatures
require, in some sense, local enrichment of embedded content via
enhanced logical forms.  \CFS\ define a model in which local
calculation is easily achieved, but it does not venture (as Grice did)
an account of how discourse participants coordinate on the right
logical forms. Each side clearly has something to contribute.  This
suggests a synthesis between the two approaches. We now turn to the
task of developing such a synthesis: a model that formally implements
pragmatic reasoning over complex, compositionally defined logical
forms and that is able to achieve the readings that seem to demand
local enrichment. The technical details of the compositional model are
different from \CFS's, and the technical details of the pragmatic
account are different from \citeauthor{Grice75}, but we hope that it
combines the best aspects of both approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The compositional lexical uncertainty model}\label{sec:model}

From \citet{Bergen:Goodman:Levy:2012} and
\citet{Bergen:Levy:Goodman:2014}. Extensions to various kinds of joint
inference: \citet{Smith:Goodman:Frank:2013} and \citet{Kao-etal:2014}.

%=====================================================================

\subsection{Grammar fragment}

\begin{examples}
\item $D$ a set of entities, $W$ a set of worlds, $\semw{$X$}$ the meaning of $X$ at $w \in W$

\item To avoid clutter, assume lowercase variables are members of $D$ and 
  uppercase variables are subsets of $D$.
\item\label{grammar}
  \newcommand{\gsem}[1]{\sem{\text{#1}}}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}[c]{r@{ $\rightarrow$ }l l}
    \toprule
    \multicolumn{2}{c}{Syntax}     & \multicolumn{1}{c}{Denotation of the left side (mother node)} \\
    \midrule
    N & person      & $\set{\tuple{w,x} : x \text{ is a person in } w}$ \\
    N & shot        & $\set{\tuple{w,x} : x \text{ is a shot in } w}$ \\
    \Vt & made      & $\set{\tuple{w, x,y} : x \text{ made } y \text{ in } w}$ \\
    \Vi & scored    & $\set{\tuple{w, x} : \exists y \ x \text{ made } y \text{ in } w}$ \\
    D & some        & $\set{\tuple{w, X, Y} : \set{x : \tuple{w,x} \in X}  \cap \set{y : \tuple{w,y} \in Y} \neq \emptyset}$ \\
    D & every       & $\set{\tuple{w, X, Y} : \set{x : \tuple{w,x} \in X}  \subseteq \set{y : \tuple{w,y} \in Y}}$ \\
    D & no          & $\set{\tuple{w, X, Y} : \set{x : \tuple{w,x} \in X}  \cap \set{y : \tuple{w,y} \in Y} = \emptyset}$ \\    
    D & exactlyone  & $\set{\tuple{w, X, Y} : |\set{x : \tuple{w,x} \in X}  \cap \set{y : \tuple{w,y} \in Y}| = 1}$ \\[1ex]
    NP & PlayerA    & $\set{\tuple{w, Y} : \playera \in \set{x : \tuple{w,x} \in Y}}$ \\
    NP & PlayerB    & $\set{\tuple{w, Y} : \playerb \in \set{x : \tuple{w,x} \in Y}}$  \\
    NP & PlayerC    & $\set{\tuple{w, Y} : \playerc \in \set{x : \tuple{w,x} \in Y}}$  \\
    NP & D N        & $\set{\tuple{w, Y} : \tuple{w, \gsem{N}, Y} \in \gsem{D}}$ \\
    VP & \Vt\ NP    & $\set{\tuple{w, x} :  \set{\tuple{w, y} :  \tuple{w, x, y} \in \gsem{\Vt}} \in \gsem{NP}}$ \\
    VP & \Vi        & $\gsem{\Vi}$ \\
    S  & NP VP      & $\set{w : \tuple{w, \gsem{VP}} \in \gsem{NP}}$ \\
    \bottomrule
  \end{tabular}

\end{examples}

%=====================================================================

\subsection{Refinement}

\begin{examples}
\item\label{refinement} Let $\Grammar$ be an interpreted grammar as in
  \eg{grammar}.  For any expression $X$ generated by $\Grammar$,
  $\Refine(X)$ is the function $f$ such that, for any world $w$,
  $f(w) = \wp(\semw{$X$}) - \emptyset$.

\item Example refinements for $D = \set{\playera,\playerb}$
 
  \begin{minipage}[t]{0.48\linewidth}
    $\semw{player} = \set{\playera}$ \\[2ex]    
    $\semw{some player} = \set{\set{\playera,\playerb}, \set{\playera}}$ \\[2ex]    
    $\Refine(\text{some player})(w) =$\\[2ex]
    $\set{
      \begin{array}[c]{l}
        \set{\set{\playera,\playerb}, \set{\playera}} \\
        \set{\set{\playera,\playerb}} \\
        \set{\set{\playera}} \\
      \end{array}
    }$    
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\linewidth}
    $\semw[w']{player} = \set{\playerb}$ \\[2ex]    
    $\semw[w']{some player} = \set{\set{\playera,\playerb}, \set{\playerb}}$ \\[2ex]    
    $\Refine(\text{some player})(w') =$ \\[2ex]
    $\set{
      \begin{array}[c]{l}
        \set{\set{\playera,\playerb}, \set{\playerb}} \\
        \set{\set{\playera,\playerb}} \\
        \set{\set{\playerb}} \\
      \end{array}
    }$    
  \end{minipage}  
\end{examples}

%=====================================================================

\subsection{Pragmatic reasoning}\label{sec:model}

\begin{examples}
\item\label{modobjects} Basic ingredients:
  \begin{examples}
  \item $\Grammar$ is an interpreted grammar as in \eg{grammar}. Let
    $\Lambda$ be the set of all well-formed expressions of $\Grammar$
    (not just the S nodes --- all of them).

  \item $\StatePrior : \Worlds \mapsto [0,1]$ is a prior probability
    distribution over worlds.

  \item $\Costs : \Messages \mapsto \Reals$ is a cost function on
    messages.  For the lexical items, costs are specified. For a
    nonterminal node $X$ with daughters $Y_{1} \ldots Y_{n}$,
    $\Costs(X) = \sum_{Y_{i}}\Costs(Y_{i})$.

  \item\label{enrichable}%
    $\Refinable \subseteq \Lambda$ is the set of refineable messages.

  \item $\Messages$ is a subset of the proposition-denoting
    expressions in $\Lambda$ augmented with a null message $\nullmsg$
    such that $\semw{$\nullmsg$} = \Worlds$ for all $w$.

  \item\label{lexset}% 
    $\LexSet = \set{\Lex' :       
      \begin{array}[c]{l}
        \Lex'(\nullmsg) = \Worlds \text{ and } \\ 
        \forall \msg \in \Refinable, \Lex'(\msg) \in \Refine(\msg) \text{ and } \\
        \forall \msg \in \Lambda{-}\Refinable, \Lex'(\msg) = \sem{\msg}
      \end{array}}$
  
  \item $\LexPrior : \LexSet \mapsto [0,1]$ is a prior probability distribution over languages.    
  
  \item $\lambda$ is a temperature parameter controlling the learning rate.
  \end{examples}

\item Agents:
  \begin{examples}
  \item\label{l0}%
    $\listenerZero(\state \given \msg, \Lex) \propto
    \frac{\mathbb{I}(\state \in \Lex(\msg))}{|\Lex(\msg)|}
    \StatePrior(\state)$

  \item\label{s1}% 
    $\speakerOne(\msg \given \state, \Lex) \propto
    \exp
    \left(
      \lambda
      \left(
        \log\left(\listenerZero(\state \given \msg, \Lex) \right)
        - 
        \Costs(\msg)
      \right)
    \right)$
    
  \item\label{l1}% 
    $\listenerOne(\state \given \msg, \Lex) \propto 
    \speakerOne(\msg \given \state, \Lex)
    \StatePrior(\state)$

  \item\label{L} 
    $\UncertaintyListener(\state \given \msg) 
    \propto 
    \StatePrior(\state)
    \sum_{\Lex \in \LexSet}
    \LexPrior(\Lex)
    \speakerOne(\msg \given \state, \Lex)$
  \end{examples}

\item We can generalize the above by allowing further iteration beyond
  $\UncertaintyListener$, but it would be nice if we could get away
  with just this form. We could also remove $\gamma$ by assuming it
  is implicitly built into $\Costs$.

\end{examples}

%=====================================================================

\subsection{Illustrations}

\newcommand{\sNN}{\texttt{NN}}
\newcommand{\sNS}{\texttt{NS}}
\newcommand{\sNA}{\texttt{NA}}

\newcommand{\sSN}{\texttt{SN}}
\newcommand{\sSS}{\texttt{SS}}
\newcommand{\sSA}{\texttt{SA}}

\newcommand{\sAN}{\texttt{AN}}
\newcommand{\sAS}{\texttt{AS}}
\newcommand{\sAA}{\texttt{AA}}

\begin{examples}
\item

 \begin{examples}
  \item $\Worlds = \set{\sNN, \sNS, \sSN, \sSS}$

    Here, for example, $\sNS$ means that $\playera$ scored and
    $\playerb$ did not.

    Implicit in this is the assumption that all the other predicates
    not connected with baskets made have the same extensions in all
    worlds.
    
  \item $\Refinable = \set{\text{some}, \text{PlayerA}, \text{PlayerB}}$
  \item $\Messages = \set{Q(\text{scored}) : Q \in \set{\text{PlayerA}, \text{PlayerB}, \text{every}, \text{no}, \text{some}}}$
  \item $\lambda = 1$. $\Costs(\nullmsg) = 5$; $\Costs(\msg) = 0$ for all $\msg \in \Messages-\set{\nullmsg}$. Flat lexicon and state priors
  %\item $\StatePrior(w) = \StatePrior(w')$ for all $w, w' \in \Worlds$
  %\item $\LexPrior(\Lex) = \LexPrior(\Lex')$ for all $\Lex, \Lex' \in \LexSet$
  \end{examples}

\item Intuitions: (i) a name in subject position should be construed
  as exhaustified, in virtue of the salience of \word{every}; (ii)
  \word{some} should be interpreted as non-specific in virtue of the
  salience of the two names; (iii) we should see the standard scalar
  implicature between \word{some} and \word{every}, in that
  \word{some} should be biased towards states in which only one player
  scored.

\item Results with the listener's best inferences based on each state
  highlighted:

  % \renewcommand{\arraystretch}{1.2}
  \setlength{\tabcolsep}{8pt}
  \begin{tabular}[c]{r *{4}{r} }
    \toprule
    & NN & NS & SN & SS\\
    \midrule
    PlayerA(scored) & 0.0 & 0.0 & \graycell{0.72} & 0.28\\
    PlayerB(scored) & 0.0 & \graycell{0.72} & 0.0 & 0.28\\
    every(player)(scored) & 0.0 & 0.0 & 0.0 & \graycell{1.0}\\
    no(player)(scored) & \graycell{1.0} & 0.0 & 0.0 & 0.0\\
    some(player)(scored) & 0.0 & \graycell{0.42} & \graycell{0.42} & 0.16\\
    NULL & 0.01 & \graycell{0.5} & \graycell{0.5} & 0.0\\
    \bottomrule
  \end{tabular}

%\item Plot of the above using roughly Mike's style for experiments:

  %\includegraphics[width=0.8\textwidth]{fig/example-simple}
\end{examples}

%=====================================================================

%\newpage

\subsubsection{Subject refinement with added predicate refinement}

\begin{examples}
\item As above except we also allow the predicate \word{scored} to be
  refined as well.

\item

  \setlength{\tabcolsep}{8pt}
  \begin{tabular}[c]{r *{4}{r} }
    \toprule
    & NN & NS & SN & SS\\
    \midrule
    PlayerA(scored) & 0.0 & 0.0 & \graycell{0.58} & 0.42\\
    PlayerB(scored) & 0.0 & \graycell{0.58} & 0.0 & 0.42\\
    every(player)(scored) & 0.0 & 0.0 & 0.0 & \graycell{1.0}\\
    no(player)(scored) & \graycell{0.59} & 0.2 & 0.2 & 0.0\\
    some(player)(scored) & 0.0 & \graycell{0.38} & \graycell{0.38} & 0.23\\
    NULL & 0.02 & \graycell{0.49} & \graycell{0.49} & 0.0\\
    \bottomrule
  \end{tabular}

\item The overall picture is the same except the results for \word{no}
  are unintuitive. Here's why: if $\sem{\text{scored}}$ can be refined
  to just $\set{\playera}$, then \texttt{no(player)(scored)} comes out
  true in world \texttt{SN}. And similarly for refinement to just
  player $\set{\playerb}$. This results in a more even distribution
  than we would like (compare the comparable line in the previous
  example).
\end{examples}

%=====================================================================

\subsubsection{Predicate scalar implicatures}

\begin{examples}
\item

 \begin{examples}
  \item $\Worlds = \set{\texttt{NN}, \texttt{NS}, \texttt{NA}, \texttt{SN}, \texttt{SS}, \texttt{SA}, \texttt{AN}, \texttt{AS}, \texttt{AA}}$        
  \item $\Refinable = \set{\text{scored}, \text{doubled}}$
  \item $\Messages = \set{Q(P) : Q \in \set{\text{PlayerA}, \text{PlayerB}}, P \in \set{\text{scored}, \text{doubled}}}$
  \item $\lambda = 1$
  \item $\Costs(\nullmsg) = 5$; $\Costs(\msg) = 0$ for all $\msg \in \Messages-\set{\nullmsg}$  
  \item Flat state prior
  \item Flat lexicon prior
  \end{examples}

\item The predicate \word{doubled} is stronger than (entails)
  \word{scored}.

\item Our expectation is that \word{scored} will be enriched to
  exclude \word{doubled} --- a standard scalar implicture.

\item This results in 194,481 viable lexica. Of course, we can obtain
  this result without lexical uncertainty, but it's still nice to see
  that the result holds even after we churn through all those lexica:

  $\mspace{-120mu}$
  \setlength{\tabcolsep}{8pt}
  \begin{tabular}[c]{r *{9}{r} }
    \toprule
    & NN & NS & NA & SN & SS & SA & AN & AS & AA\\
    \midrule
    PlayerA(scored) & 0.0 & 0.0 & 0.0 & \graycell{0.27} & 0.2 & 0.13 & 0.17 & 0.13 & 0.1\\
    PlayerB(scored) & 0.0 & \graycell{0.27} & 0.17 & 0.0 & 0.2 & 0.13 & 0.0 & 0.13 & 0.1\\
    PlayerA(doubled) & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & \graycell{0.4} & 0.34 & 0.26\\
    PlayerB(doubled) & 0.0 & 0.0 & \graycell{0.4} & 0.0 & 0.0 & 0.34 & 0.0 & 0.0 & 0.26\\
    NULL & \graycell{0.34} & 0.17 & 0.07 & 0.17 & 0.08 & 0.04 & 0.07 & 0.04 & 0.02\\
    \bottomrule
  \end{tabular}
  
\end{examples}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiment 1: scalars under quantifiers}\label{sec:exp1}

Rationale: improve upon \citet{Chemla:Spector:2011} by using better
items, more normal response patterns, and more standard assumptions
about what the responses mean. Also perhaps make it easier to open
things up to additional manipulations (e.g., QUDs).

%=====================================================================

\subsection{Materials, methods, and participants}

%=====================================================================

\subsection{Analysis}

\begin{examples}
\item

 \begin{examples}
  \item $\Worlds = \set{\texttt{NNN}, \texttt{NNS}, \texttt{NNA}, \texttt{NSS}, \texttt{NSA}, \texttt{NAA}, \texttt{SSS}, \texttt{SSA}, \texttt{SAA}, \texttt{AAA}}$    
  \item $\Refinable = \set{\text{some player}, \text{some shot}}$
  \item $\Messages = \set{Q(\text{player})(\text{made}(S(\text{shot}))) : Q \in \set{\text{exactlyone}, \text{every}, \text{no}}, S \in  \set{\text{every}, \text{no}, \text{some}}}$
  \item $\lambda = 1$
  \item $\Costs(\nullmsg) = 5$; $\Costs(\msg) = 0$ for all $\msg \in \Messages-\set{\nullmsg}$  
  \item $\StatePrior(w) = \StatePrior(w')$ for all $w, w' \in \Worlds$
  \item $\LexPrior(\Lex) = \LexPrior(\Lex')$ for all $\Lex, \Lex' \in \LexSet$
  \end{examples}

\item Goals given our experimental findings:
  \begin{examples}
  \item \texttt{every(player)(made(some(shot)))} should strongly favor \texttt{SSS}
  \item \texttt{exactlyone(player)(made(some(shot)))} should weakly favor \texttt{NNS}, but \texttt{SAA} and \texttt{NSA} should be contenders.
    (\texttt{NNA} is the unenriched case.)
  \end{examples}


\item Results with the listener's best inferences based on each state
  highlighted:

  $\mspace{-120mu}$
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}[c]{r *{10}{r} }
    \toprule
    & NNN & NNS & NNA & NSS & NSA & NAA & SSS & SSA & SAA & AAA\\
    \midrule
    every(player)(made(every(shot))) & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & \graycell{1.0}\\
    every(player)(made(no(shot))) & \graycell{1.0} & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\
    every(player)(made(some(shot))) & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & \graycell{0.41} & 0.12 & 0.21 & 0.25\\
    exactlyone(player)(made(every(shot))) & 0.0 & 0.0 & \graycell{0.37} & 0.0 & 0.29 & 0.0 & 0.0 & 0.34 & 0.0 & 0.0\\
    exactlyone(player)(made(no(shot))) & 0.0 & 0.0 & 0.0 & 0.29 & 0.22 & \graycell{0.49} & 0.0 & 0.0 & 0.0 & 0.0\\
    exactlyone(player)(made(some(shot))) & 0.0 & \graycell{0.31} & 0.28 & 0.0 & 0.17 & 0.0 & 0.0 & 0.09 & 0.15 & 0.0\\
    no(player)(made(every(shot))) & 0.12 & \graycell{0.34} & 0.0 & 0.31 & 0.0 & 0.0 & 0.23 & 0.0 & 0.0 & 0.0\\
    no(player)(made(no(shot))) & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.2 & 0.24 & \graycell{0.46} & 0.11\\
    no(player)(made(some(shot))) & \graycell{0.27} & 0.17 & 0.15 & 0.1 & 0.0 & 0.15 & 0.11 & 0.0 & 0.0 & 0.06\\
    NULL & 0.04 & 0.11 & 0.1 & 0.11 & 0.08 & \graycell{0.18} & 0.08 & 0.09 & \graycell{0.18} & 0.04\\
    \bottomrule
  \end{tabular}

\item Result also holds if \word{some} is allowed in the subject.

\item If \word{exactly one} is allowed in the object, then we still
  see the \word{every}/\word{some} result, but we lose the
  \word{exactly one}/\word{some} implicature. I think that's very
  intuitive --- if the speaker avoided salient and (in the above)
  equally costly \word{exactly one}, then she surely didn't intend its
  meaning.
  
\newpage

\item Plot of the above with the experimental results:

  $\mspace{-120mu}$
  \includegraphics[width=1.2\textwidth]{fig/experiment-barplots}


\newpage

\item With the human judgments normalized as listener judgments:

  $\mspace{-120mu}$
  \includegraphics[width=1.2\textwidth]{fig/experiment-barplots-listenernorm}

\newpage

\item With the human judgments normalized as speaker judgments:

  $\mspace{-120mu}$
  \includegraphics[width=1.2\textwidth]{fig/experiment-barplots-speakernorm}

\newpage

\item Correlation plot with the human data normalized as though it
  were a \textbf{listener} distribution (a simple rescaling of the
  raw data):

  \includegraphics[width=1\textwidth]{fig/experiment-scatterplot-listenernorm.pdf}


\item Correlation plot with the human data normalized as though it
  were a \textbf{speaker} distribution:

  \includegraphics[width=1\textwidth]{fig/experiment-scatterplot-speakernorm.pdf}
\end{examples}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiment 2: explicit exhaustification}\label{sec:exp2}

\begin{examples}
\item Rationale: \CFS\ use an operator that shares many
  characteristics with \word{only}. (This is part of the theoretical
  appeal, in that the operator seems to be needed anyway.)  Adding an
  explicit \word{only} gives us a sense for the pattern with explicit
  signaling --- a kind of upperbound on the effects we would expect.

\item Relevance for our model: adding \word{only} constrains the space
  of alternatives in ways that boost the strength of the embedding and
  basically/fully remove the need for pragmatic reasoning.  
\end{examples}

%=====================================================================

\subsection{Materials, methods, and participants}

%=====================================================================

\subsection{Analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiment 3: prosodic focus}\label{sec:exp3}

\begin{examples}
\item Rationale: \CFS's operator is also closely related to what one
  expects on accounts of topic and focus intonation that are based in
  alternatives. The \ALT\ operator embodies one of many things that
  one might do pragmatically with alternatives (negate them; in other
  contexts, they could be affirmed, highlighted because of speaker
  ignorance, etc.). Thus, we might expect prosodic focus to boost the
  signal.

\item Relevance for our model: harder to say. We could assume that
  focus alternatives are somehow part of the messages. We could also
  see what happens when focal forms are simply made more costly (in
  terms of the cost function $\Costs$) than their unfocussed
  counterparts.
\end{examples}

%=====================================================================

\subsection{Materials, methods, and participants}

%=====================================================================

\subsection{Analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apalike}
\bibliography{embedded-scalars-bib}


\end{document}

